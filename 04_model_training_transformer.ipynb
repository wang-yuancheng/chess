{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b389472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import chess\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" \n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"run_2026_01_18_probs_seresnet_droppath_c32_v12\"\n",
    "model_save_name = \"probs_seresnet_droppath_c32_v12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39910bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, root_dir: Path, split: str, sigma: float = 0.6):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.sigma = sigma\n",
    "        self.num_classes = 7\n",
    "        self.class_indices = torch.arange(self.num_classes, dtype=torch.float32)\n",
    "        self.X = np.load(self.root_dir / f\"{self.split}_X.npy\", mmap_mode='r')\n",
    "        self.y = np.load(self.root_dir / f\"{self.split}_y.npy\", mmap_mode='r')\n",
    "        self.scores = np.load(self.root_dir / f\"{self.split}_scores.npy\", mmap_mode='r')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def score_to_continuous_index(self, score: float) -> float:\n",
    "        \"\"\"\n",
    "        Maps Centipawn score to a continuous index (e.g. 400cp -> 1.5).\n",
    "        \"\"\"\n",
    "        \n",
    "        if score >= 500: \n",
    "            # Fade from 0.5 (at 500) to 0.0 (at 700)\n",
    "            return max(0.0, 0.5 - (score - 500) / 200.0)\n",
    "        \n",
    "        if score <= -500:\n",
    "            # Fade from 5.5 (at -500) to 6.0 (at -700)\n",
    "            return min(6.0, 5.5 + (-500 - score) / 200.0)\n",
    "        \n",
    "        # Interpolate the Middle Classes\n",
    "        # 300 to 500  -> Maps to 1.5 to 0.5\n",
    "        if score >= 300: return 1.5 - (score - 300) / 200.0\n",
    "        # 100 to 300  -> Maps to 2.5 to 1.5\n",
    "        if score >= 100: return 2.5 - (score - 100) / 200.0\n",
    "        # -100 to 100 -> Maps to 3.5 to 2.5\n",
    "        if score >= -100: return 3.5 - (score - (-100)) / 200.0\n",
    "        # -300 to -100 -> Maps to 4.5 to 3.5\n",
    "        if score >= -300: return 4.5 - (score - (-300)) / 200.0\n",
    "        # -500 to -300 -> Maps to 5.5 to 4.5\n",
    "        if score > -500: return 5.5 - (score - (-500)) / 200.0\n",
    "        \n",
    "        return 3.0 \n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        score = self.scores[idx].item()\n",
    "        target_idx = self.score_to_continuous_index(score)\n",
    "        \n",
    "        # Create Gaussian Distribution centered at target_idx\n",
    "        dist = torch.exp(-((self.class_indices - target_idx) ** 2) / (2 * self.sigma ** 2))\n",
    "        \n",
    "        # Normalize so it sums to 1.0\n",
    "        soft_target = dist / dist.sum()\n",
    "        \n",
    "        x_tensor = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        hard_label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "        # return self.X[idx], self.y[idx]\n",
    "        return x_tensor, soft_target, hard_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "num_workers = 0 \n",
    "ROOT_DIR = Path(\"./dataset_planes_cp/\")\n",
    "\n",
    "train_dataset = ChessDataset(root_dir=ROOT_DIR, split=\"train\")\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "\n",
    "val_dataset = ChessDataset(root_dir=ROOT_DIR, split=\"val\")\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True)\n",
    "\n",
    "test_dataset = ChessDataset(root_dir=ROOT_DIR, split=\"test\")\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i, (X, y, y_hard) in enumerate(train_dataloader):\n",
    "    if i == 100:  # measure 100 batches\n",
    "        break\n",
    "print(\"Avg batch load time:\", (time.time() - start) / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, _, yb = next(iter(train_dataloader))\n",
    "print(\"X batch shape:\", Xb.shape, \"dtype:\", Xb.dtype)\n",
    "print(\"y batch shape:\", yb.shape, \"dtype:\", yb.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionLabel(Enum):\n",
    "    WHITE_WINNING = 0\n",
    "    WHITE_DECISIVE = 1\n",
    "    WHITE_BETTER = 2\n",
    "    EQUAL = 3\n",
    "    BLACK_BETTER = 4\n",
    "    BLACK_DECISIVE = 5\n",
    "    BLACK_WINNING = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Ffn(nn.Module):\n",
    "    \"\"\"\n",
    "    Corresponds to 'shared.py'.\n",
    "    A standard MLP: Linear -> Activation -> Linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.activation = nn.ReLU() \n",
    "        self.linear2 = nn.Linear(ffn_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Corresponds to 'encoder.py'.\n",
    "    Implements the specific 'DeepNorm' Pre-LN/Post-LN hybrid used by Leela.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, num_blocks):\n",
    "        super().__init__()\n",
    "        # Leela uses a specific scaling factor 'alpha' for stability\n",
    "        self.alpha = math.pow(2.0 * num_blocks, -0.25)\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.ffn = Ffn(embed_dim, ffn_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Attention with Residual Scaling\n",
    "        # Note: Leela adds residual *before* LayerNorm (Post-Norm variant)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = x + attn_out * self.alpha\n",
    "        \n",
    "        # 2. First LayerNorm\n",
    "        out1 = self.ln1(x)\n",
    "        \n",
    "        # 3. FFN with Residual Scaling\n",
    "        ffn_out = self.ffn(out1)\n",
    "        \n",
    "        # 4. Second LayerNorm (on the residual sum)\n",
    "        out2 = self.ln2(out1 + ffn_out * self.alpha)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The full model.\n",
    "    Input: (Batch, 112, 8, 8)\n",
    "    Output: (Batch, 7)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=112, embed_dim=256, num_heads=8, ffn_dim=1024, num_blocks=10, num_classes=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embedding: Flatten 8x8 -> 64 tokens, project channels to embed_dim\n",
    "        self.embedding = nn.Linear(input_channels, embed_dim)\n",
    "        \n",
    "        # 2. Transformer Tower\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(embed_dim, num_heads, ffn_dim, num_blocks)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # 3. Classifier Head (Your custom 7-class output)\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (Batch, 112, 8, 8)\n",
    "        \n",
    "        # Step A: Reshape to Sequence\n",
    "        # Permute to (Batch, 8, 8, 112) then flatten to (Batch, 64, 112)\n",
    "        x = x.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "        \n",
    "        # Step B: Embed\n",
    "        x = self.embedding(x) # Shape: (Batch, 64, embed_dim)\n",
    "        \n",
    "        # Step C: Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # Step D: Global Pooling\n",
    "        # Average all 64 squares to get one vector for the board\n",
    "        x = x.mean(dim=1) # Shape: (Batch, embed_dim)\n",
    "        \n",
    "        # Step E: Classify\n",
    "        logits = self.classifier_head(x) # Shape: (Batch, 7)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    input_channels=112,  \n",
    "    embed_dim=256,      # Size of the vector for each square\n",
    "    num_heads=8,\n",
    "    ffn_dim=512,\n",
    "    num_blocks=6,       # Number of transformer layers\n",
    "    num_classes=7    \n",
    ")\n",
    "\n",
    "# dummy_input = torch.randn(16, 112, 8, 8)\n",
    "\n",
    "# output = model(dummy_input)\n",
    "\n",
    "# print(f\"Input shape: {dummy_input.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\") # Should be [16, 7]\n",
    "\n",
    "# # Get predictions\n",
    "# probabilities = torch.softmax(output, dim=1)\n",
    "# predicted_class = torch.argmax(probabilities, dim=1)\n",
    "# print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               scaler: torch.amp.GradScaler,\n",
    "               device=device) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Performs one training epoch for the given model.\n",
    "    Returns the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y, _) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device) # X and Y are both shape (BATCH_SIZE,)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass\n",
    "        with torch.amp.autocast(device):\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy metrics\n",
    "        \"\"\"softmax and argmax dim=1 because tensor of shape (batchsize, num_classes)\"\"\"\n",
    "        y_pred_class = torch.argmax(y_pred, dim=-1) # y_pred_class.shape = (BATCH_SIZE,)\n",
    "        # train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "        # Remove for non prob ablation\n",
    "        y_true_class = torch.argmax(y, dim=-1)\n",
    "        train_acc += (y_pred_class == y_true_class).sum().item()/len(y_pred)\n",
    "        \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56112dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the given dataloader without gradient updates.\n",
    "    Dataloader should either be the validation or test dataloader.\n",
    "    Returns the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y, y_hard) in enumerate(dataloader):\n",
    "            X, y, y_hard = X.to(device), y.to(device), y_hard.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy metrics\n",
    "            test_pred_labels = torch.argmax(test_pred, dim=1)\n",
    "            test_acc += (test_pred_labels == y_hard).sum().item() / len(test_pred_labels)\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121015af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "def run_experiment(model: torch.nn.Module,\n",
    "                   model_save_name: str,\n",
    "                   train_dataloader: torch.utils.data.DataLoader,\n",
    "                   val_dataloader: torch.utils.data.DataLoader,\n",
    "                   loss_fn: torch.nn.Module,\n",
    "                   optimizer: torch.optim.Optimizer,\n",
    "                   scaler: torch.amp.GradScaler,\n",
    "                   epochs: int,\n",
    "                   patience: int,\n",
    "                   device=device):\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": []}\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_weights = None\n",
    "    patience_counter = 0 \n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer, mode='min', factor=0.1, patience=3\n",
    "    # )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=epochs,      \n",
    "        eta_min=1e-6         \n",
    "    )\n",
    "    \n",
    "    print(f\"Starting Training: {model_save_name}\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device,\n",
    "                                           scaler=scaler)\n",
    "        val_loss, val_acc = eval_step(model=model,\n",
    "                                      dataloader=val_dataloader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      device=device)\n",
    "        \n",
    "        # scheduler.step(val_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc: \n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            \n",
    "            print(f\"Epoch: {epoch} | New Best Val Acc: {val_acc:.4f} (Saved)\")\n",
    "            torch.save(model.state_dict(), f\"models/{model_save_name}.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Epoch: No improvement. Patience {patience_counter}/{patience}\")\n",
    "\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n[Early Stopping] No improvement for {patience} epochs. Stopping.\")\n",
    "            break \n",
    "\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"\\nLoaded best model weights with Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = Transformer(in_channels=19, \n",
    "                 channels=64, \n",
    "                 num_blocks=10, \n",
    "                 num_classes=7).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 19, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                              lr=0.001,\n",
    "                              weight_decay=0.01)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "result = run_experiment(model=model,\n",
    "                        model_save_name=model_save_name,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        val_dataloader=val_dataloader,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optimizer,\n",
    "                        scaler=scaler,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        patience=NUM_EPOCHS + 1,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23488d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index = {\"P\":0,\n",
    "                  \"N\":1,\n",
    "                  \"B\":2,\n",
    "                  \"R\":3,\n",
    "                  \"Q\":4,\n",
    "                  \"K\":5,\n",
    "                  \"p\":6,\n",
    "                  \"n\":7,\n",
    "                  \"b\":8,\n",
    "                  \"r\":9,\n",
    "                  \"q\":10,\n",
    "                  \"k\":11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d13910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_tensor(fen):\n",
    "    \"\"\"\n",
    "    Converts FEN into a (19, 8, 8) tensor.\n",
    "    \"\"\"\n",
    "    board = chess.Board(fen)\n",
    "    tensor = np.zeros((19, 8, 8), dtype=np.uint8)\n",
    "\n",
    "    piece_to_channel = {\n",
    "        \"P\": 0, \"N\": 1, \"B\": 2, \"R\": 3, \"Q\": 4, \"K\": 5,\n",
    "        \"p\": 6, \"n\": 7, \"b\": 8, \"r\": 9, \"q\": 10, \"k\": 11\n",
    "    }\n",
    "\n",
    "    for square, piece in board.piece_map().items():\n",
    "        channel = piece_to_channel[piece.symbol()]\n",
    "        row, col = divmod(square, 8)\n",
    "        tensor[channel, row, col] = 1\n",
    "\n",
    "    \n",
    "    if board.turn == chess.WHITE:\n",
    "        tensor[12, :, :] = 1\n",
    "    if board.has_kingside_castling_rights(chess.WHITE):\n",
    "        tensor[13, :, :] = 1\n",
    "    if board.has_queenside_castling_rights(chess.WHITE):\n",
    "        tensor[14, :, :] = 1\n",
    "    if board.has_kingside_castling_rights(chess.BLACK):\n",
    "        tensor[15, :, :] = 1\n",
    "    if board.has_queenside_castling_rights(chess.BLACK):\n",
    "        tensor[16, :, :] = 1\n",
    "    if board.is_check():\n",
    "        tensor[17, :, :] = 1\n",
    "\n",
    "    if board.ep_square is not None:\n",
    "        row, col = divmod(board.ep_square, 8)\n",
    "        tensor[18, row, col] = 1\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e936bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_prediction(model: torch.nn.Module,\n",
    "                           random_fen: str,\n",
    "                           fen_class: int,\n",
    "                           device=device):\n",
    "    \"\"\"Takes the given fen to see the board, predicted score and actual score\"\"\"\n",
    "\n",
    "    numpy_fen = fen_to_tensor(random_fen)\n",
    "    torch_fen = torch.tensor(numpy_fen, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = torch.argmax(model(torch_fen), dim=-1)\n",
    "    print(\"Model Prediction: \", pred.item())\n",
    "    print(\"Stockfish Evaluation: \", fen_class)\n",
    "\n",
    "# Remember to update fen_class manually\n",
    "random_fen = \"r3kb1r/p2b1pp1/2p1pq1p/P2n4/3P4/1Q3N2/1PP2PPP/R1B2RK1 b kq - 2 14\"\n",
    "check_model_prediction(model=model,\n",
    "                       random_fen=random_fen,\n",
    "                       fen_class=5, # remember to manually set \n",
    "                       device=device)\n",
    "\n",
    "board = chess.Board(random_fen)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y, y_hard in val_dataloader:\n",
    "        X = X.to(device)\n",
    "        preds = model(X).argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_hard.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efa820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "LOGS_DIR = f\"experiments/logs/{model_save_name}\"\n",
    "\n",
    "def save_config_metadata(experiment_name: str, \n",
    "                         model: torch.nn.Module, \n",
    "                         hyperparams: dict, \n",
    "                         dataset_paths: dict,\n",
    "                         save_dir: str = LOGS_DIR):\n",
    "    \"\"\"\n",
    "    Saves all 'static' setup details: Model architecture, parameter counts, \n",
    "    datasets used, and hyperparameters.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Model Metadata\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    config_data = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_architecture\": {\n",
    "            \"class_name\": model.__class__.__name__,\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"structure_summary\": str(model)\n",
    "        },\n",
    "        \"datasets\": dataset_paths,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"device\": torch.cuda.get_device_name() if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "\n",
    "    file_path = f\"{save_dir}/{experiment_name}_config.json\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(config_data, f, indent=4)\n",
    "    \n",
    "    print(f\"[Config] Saved metadata to {file_path}\")\n",
    "    \n",
    "def save_training_logs(experiment_name: str, \n",
    "                       results_dict: dict, \n",
    "                       save_dir: str = LOGS_DIR):\n",
    "    \"\"\"\n",
    "    Saves the epoch-by-epoch learning curves (Loss/Acc) to CSV.\n",
    "    Expects results_dict to be the output from your run_experiment function.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame(results_dict)\n",
    "    \n",
    "    if \"epoch\" not in df.columns:\n",
    "        df[\"epoch\"] = range(1, len(df) + 1)\n",
    "        \n",
    "    file_path = f\"{save_dir}/{experiment_name}_learning_curves.csv\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"[Logs] Saved training history to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "RESULTS_DIR = f\"experiments/results/{model_save_name}\"\n",
    "\n",
    "def calculate_ordinal_metrics(preds: np.ndarray,\n",
    "                              labels: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates metrics specific to ordinal classification (where Class 0 is close to Class 1).\n",
    "    \"\"\"\n",
    "    abs_diffs = np.abs(preds - labels)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mae\": float(np.mean(abs_diffs)),\n",
    "        \"off_by_one_accuracy\": float(np.mean(abs_diffs <= 1)),\n",
    "        \"off_by_two_accuracy\": float(np.mean(abs_diffs <= 2))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def categorize_failures(preds: np.ndarray, \n",
    "                        labels: np.ndarray) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Categorizes errors by magnitude.\n",
    "    Returns a dict where keys are the error magnitude (3, 4, 5, 6) and values are lists of dataset indices.\n",
    "    \"\"\"\n",
    "    abs_diffs = np.abs(preds - labels)\n",
    "    failure_dict = {}\n",
    "    \n",
    "    # We care about errors >= 3 (e.g. Predicting 'Equal' when 'Black Winning')\n",
    "    # Max error is 6 (Predicting 'White Winning' when 'Black Winning')\n",
    "    for magnitude in range(3, 7):\n",
    "        indices = np.where(abs_diffs == magnitude)[0].tolist()\n",
    "        if indices:\n",
    "            failure_dict[magnitude] = indices\n",
    "            \n",
    "    return failure_dict\n",
    "\n",
    "def run_inference(model: torch.nn.Module, \n",
    "                  dataloader: torch.utils.data.DataLoader, \n",
    "                  device: str) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Runs inference and tracks latency. Returns predictions, true labels, and avg latency per sample (ms).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, _, y_hard in dataloader:\n",
    "            X = X.to(device)\n",
    "            preds = model(X).argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_hard.numpy())\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    num_samples = len(all_labels)\n",
    "    avg_latency_ms = (total_time / num_samples) * 1000\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), avg_latency_ms\n",
    "\n",
    "def save_test_results(experiment_name: str, \n",
    "                      model: torch.nn.Module, \n",
    "                      test_dataloader: torch.utils.data.DataLoader, \n",
    "                      device: str,\n",
    "                      save_dir: str = RESULTS_DIR):\n",
    "    \"\"\"\n",
    "    Orchestrates the testing process and saves all research-grade metrics.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    preds, labels, latency_ms = run_inference(model, test_dataloader, device)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    class_report = classification_report(labels, preds, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    ordinal_metrics = calculate_ordinal_metrics(preds, labels)\n",
    "    failure_indices = categorize_failures(preds, labels)\n",
    "    \n",
    "    final_metrics = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"global_accuracy\": acc,\n",
    "        \"inference_latency_ms\": latency_ms,\n",
    "        \"ordinal_metrics\": ordinal_metrics,\n",
    "        \"catastrophic_failure_counts\": {k: len(v) for k, v in failure_indices.items()},\n",
    "        \"classification_report\": class_report\n",
    "    }\n",
    "\n",
    "    json_path = f\"{save_dir}/{experiment_name}_metrics.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(final_metrics, f, indent=4)\n",
    "        \n",
    "    npy_path = f\"{save_dir}/{experiment_name}_confusion_matrix.npy\"\n",
    "    np.save(npy_path, conf_matrix)\n",
    "    \n",
    "    # Failure Indices JSON (for later visual analysis of specific FENs)\n",
    "    failures_path = f\"{save_dir}/{experiment_name}_failure_indices.json\"\n",
    "    with open(failures_path, \"w\") as f:\n",
    "        json.dump(failure_indices, f)\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"[Results] Accuracy:        {acc*100:.2f}%\")\n",
    "    print(f\"[Results] Off-by-1 Acc:    {ordinal_metrics['off_by_one_accuracy']*100:.2f}%\")\n",
    "    print(f\"[Results] MAE:             {ordinal_metrics['mae']:.4f}\")\n",
    "    print(f\"[Results] Latency:         {latency_ms:.4f} ms/sample\")\n",
    "    print(\"[Results] Catastrophic Failures (Count):\")\n",
    "    for k in sorted(failure_indices.keys()):\n",
    "        print(f\"   - Off by {k}: {len(failure_indices[k])} samples\")\n",
    "    print(f\"[Results] Saved all metrics to {save_dir}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed637bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "}\n",
    "\n",
    "dataset_paths = {\n",
    "    \"train\": str(ROOT_DIR / \"train_X.npy\"),\n",
    "    \"val\":   str(ROOT_DIR / \"val_X.npy\"),\n",
    "    \"test\":  str(ROOT_DIR / \"test_X.npy\")\n",
    "}\n",
    "\n",
    "save_config_metadata(experiment_name=RUN_ID,\n",
    "                     model=model,\n",
    "                     hyperparams=hyperparams,\n",
    "                     dataset_paths=dataset_paths)\n",
    "\n",
    "# Save Training Logs (Using the 'result' variable from run_experiment)\n",
    "save_training_logs(experiment_name=RUN_ID, \n",
    "                   results_dict=result)\n",
    "\n",
    "save_test_results(experiment_name=RUN_ID,\n",
    "                  model=model,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = Path(f\"./experiments/logs/{model_save_name}/{RUN_ID}_learning_curves.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['train_loss'], label='Train Loss', color='blue')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Val Loss', color='orange')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['train_acc'], label='Train Accuracy', color='blue')\n",
    "plt.plot(df['epoch'], df['val_acc'], label='Val Accuracy', color='orange')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "chessenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
