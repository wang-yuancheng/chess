{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b389472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import chess\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" \n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"run_2026_01_02_wide_mlp_v1\"\n",
    "model_save_name = \"wide_mlp_v1\"\n",
    "# Need to mirror positions for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39910bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZChessDataset(Dataset):\n",
    "    def __init__(self, npz_path: Path):\n",
    "        with np.load(npz_path) as data:\n",
    "            self.X = torch.tensor(data[\"X\"], dtype=torch.float32) # float32 for NNs\n",
    "            self.y = torch.tensor(data[\"y\"], dtype=torch.long) # long for CrossEntropyLoss\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "num_workers = 0\n",
    "\n",
    "TRAIN_PATH = Path(\"./dataset_processed/bitboard_train.npz\")\n",
    "VAL_PATH = Path(\"./dataset_processed/bitboard_val.npz\")\n",
    "TEST_PATH = Path(\"./dataset_processed/bitboard_test.npz\") \n",
    "\n",
    "train_dataloader = DataLoader(dataset=NPZChessDataset(TRAIN_PATH), \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "val_dataloader = DataLoader(dataset=NPZChessDataset(VAL_PATH), \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True)\n",
    "test_dataloader = DataLoader(dataset=NPZChessDataset(TEST_PATH), \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i, (X, y) in enumerate(train_dataloader):\n",
    "    if i == 100:  # measure 100 batches\n",
    "        break\n",
    "print(\"Avg batch load time:\", (time.time() - start) / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, yb = next(iter(train_dataloader))\n",
    "print(\"X batch shape:\", Xb.shape, \"dtype:\", Xb.dtype)\n",
    "print(\"y batch shape:\", yb.shape, \"dtype:\", yb.dtype, \"classes in batch:\", yb.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionLabel(Enum):\n",
    "    WHITE_WINNING = 0\n",
    "    WHITE_DECISIVE = 1\n",
    "    WHITE_BETTER = 2\n",
    "    EQUAL = 3\n",
    "    BLACK_BETTER = 4\n",
    "    BLACK_DECISIVE = 5\n",
    "    BLACK_WINNING = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_units, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Performs one training epoch for the given model.\n",
    "    Returns the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device) # X and Y are both shape (BATCH_SIZE,)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy metrics\n",
    "        \"\"\"softmax and argmax dim=1 because tensor of shape (batchsize, num_classes)\"\"\"\n",
    "        y_pred_class = torch.argmax(y_pred, dim=-1) # y_pred_class.shape = (BATCH_SIZE,)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56112dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the given dataloader without gradient updates.\n",
    "    Dataloader should either be the validation or test dataloader.\n",
    "    Returns the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy metrics\n",
    "            test_pred_labels = torch.argmax(test_pred, dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item()/len(test_pred_labels)\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121015af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "def run_experiment(model: torch.nn.Module,\n",
    "                   model_save_name: str,\n",
    "                   train_dataloader: torch.utils.data.DataLoader,\n",
    "                   val_dataloader: torch.utils.data.DataLoader,\n",
    "                   loss_fn: torch.nn.Module,\n",
    "                   optimizer: torch.optim.Optimizer,\n",
    "                   epochs: int,\n",
    "                   device=device):\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": []}\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_weights = None\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        \n",
    "        val_loss, val_acc = eval_step(model=model,\n",
    "                                      dataloader=val_dataloader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      device=device)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            print(f\"Epoch {epoch}: New Best Val Loss: {val_loss:.4f} (Saved)\")\n",
    "            torch.save(model.state_dict(), f\"{model_save_name}.pth\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            \n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss} | Train Acc: {train_acc} | Val Loss: {val_loss} | Val Acc: {val_acc}\")\n",
    "\n",
    "        # To plot results later\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"\\nLoaded best model weights with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = WideMLP(input_shape=775,\n",
    "                        hidden_units=2048,\n",
    "                        output_shape=7).to(device)\n",
    "\n",
    "summary(model, input_size=(2048, 775))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                              lr=0.001)\n",
    "\n",
    "result = run_experiment(model=model,\n",
    "                        model_save_name=model_save_name,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        val_dataloader=val_dataloader,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23488d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index = {\"P\":0,\n",
    "                  \"N\":1,\n",
    "                  \"B\":2,\n",
    "                  \"R\":3,\n",
    "                  \"Q\":4,\n",
    "                  \"K\":5,\n",
    "                  \"p\":6,\n",
    "                  \"n\":7,\n",
    "                  \"b\":8,\n",
    "                  \"r\":9,\n",
    "                  \"q\":10,\n",
    "                  \"k\":11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d13910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_vector(fen: str, is_check: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts FEN to a 775-dim vector (Bitboards + Game State).\n",
    "    \"\"\"\n",
    "    board = chess.Board(fen)\n",
    "    vector = np.zeros(775, dtype=np.uint8)\n",
    "    \n",
    "    for square, piece in board.piece_map().items():\n",
    "        idx = piece_to_index[piece.symbol()] * 64 + square\n",
    "        vector[idx] = 1\n",
    "\n",
    "    \n",
    "    # Side to Move (1 = White, 0 = Black)\n",
    "    vector[768] = 1.0 if board.turn == chess.WHITE else 0.0\n",
    "    \n",
    "    # Castling Rights\n",
    "    vector[769] = 1.0 if board.has_kingside_castling_rights(chess.WHITE) else 0.0\n",
    "    vector[770] = 1.0 if board.has_queenside_castling_rights(chess.WHITE) else 0.0\n",
    "    vector[771] = 1.0 if board.has_kingside_castling_rights(chess.BLACK) else 0.0\n",
    "    vector[772] = 1.0 if board.has_queenside_castling_rights(chess.BLACK) else 0.0\n",
    "    \n",
    "    # If there is an en-passant square target, set to 1\n",
    "    vector[773] = 1.0 if board.ep_square is not None else 0.0\n",
    "\n",
    "    # Is there a check?\n",
    "    vector[774] = 1.0 if is_check else 0.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e936bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_prediction(model: torch.nn.Module,\n",
    "                           random_fen: str,\n",
    "                           fen_class: int,\n",
    "                           device=device):\n",
    "    \"\"\"Takes the given fen to see the board, predicted score and actual score\"\"\"\n",
    "\n",
    "    numpy_fen = fen_to_vector(random_fen)\n",
    "    torch_fen = torch.tensor(numpy_fen, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = torch.argmax(model(torch_fen), dim=-1)\n",
    "    print(\"Model Prediction: \", pred.item())\n",
    "    print(\"Stockfish Evaluation: \", fen_class)\n",
    "\n",
    "# Remember to update fen_class manually\n",
    "random_fen = \"r3kb1r/p2b1pp1/2p1pq1p/P2n4/3P4/1Q3N2/1PP2PPP/R1B2RK1 b kq - 2 14\"\n",
    "check_model_prediction(model=model,\n",
    "                       random_fen=random_fen,\n",
    "                       fen_class=5, # remember to manually set \n",
    "                       device=device)\n",
    "\n",
    "board = chess.Board(random_fen)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y in val_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        preds = model(X).argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efa820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def save_config_metadata(experiment_name: str, \n",
    "                         model: torch.nn.Module, \n",
    "                         hyperparams: dict, \n",
    "                         dataset_paths: dict,\n",
    "                         save_dir: str = \"experiments/logs\"):\n",
    "    \"\"\"\n",
    "    Saves all 'static' setup details: Model architecture, parameter counts, \n",
    "    datasets used, and hyperparameters.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Model Metadata\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    config_data = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_architecture\": {\n",
    "            \"class_name\": model.__class__.__name__,\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"input_dim\": hyperparams.get(\"input_shape\", \"unknown\"),\n",
    "            \"output_dim\": hyperparams.get(\"output_shape\", \"unknown\"),\n",
    "            \"structure_summary\": str(model)\n",
    "        },\n",
    "        \"datasets\": dataset_paths,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"device\": torch.cuda.get_device_name() if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "\n",
    "    file_path = f\"{save_dir}/{experiment_name}_config.json\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(config_data, f, indent=4)\n",
    "    \n",
    "    print(f\"[Config] Saved metadata to {file_path}\")\n",
    "    \n",
    "def save_training_logs(experiment_name: str, \n",
    "                       results_dict: dict, \n",
    "                       save_dir: str = \"experiments/logs\"):\n",
    "    \"\"\"\n",
    "    Saves the epoch-by-epoch learning curves (Loss/Acc) to CSV.\n",
    "    Expects results_dict to be the output from your run_experiment function.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame(results_dict)\n",
    "    \n",
    "    if \"epoch\" not in df.columns:\n",
    "        df[\"epoch\"] = range(1, len(df) + 1)\n",
    "        \n",
    "    file_path = f\"{save_dir}/{experiment_name}_learning_curves.csv\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"[Logs] Saved training history to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def calculate_ordinal_metrics(preds: np.ndarray,\n",
    "                              labels: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates metrics specific to ordinal classification (where Class 0 is close to Class 1).\n",
    "    \"\"\"\n",
    "    abs_diffs = np.abs(preds - labels)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mae\": float(np.mean(abs_diffs)),\n",
    "        \"off_by_one_accuracy\": float(np.mean(abs_diffs <= 1)),\n",
    "        \"off_by_two_accuracy\": float(np.mean(abs_diffs <= 2))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def categorize_failures(preds: np.ndarray, \n",
    "                        labels: np.ndarray) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Categorizes errors by magnitude.\n",
    "    Returns a dict where keys are the error magnitude (3, 4, 5, 6) and values are lists of dataset indices.\n",
    "    \"\"\"\n",
    "    abs_diffs = np.abs(preds - labels)\n",
    "    failure_dict = {}\n",
    "    \n",
    "    # We care about errors >= 3 (e.g. Predicting 'Equal' when 'Black Winning')\n",
    "    # Max error is 6 (Predicting 'White Winning' when 'Black Winning')\n",
    "    for magnitude in range(3, 7):\n",
    "        indices = np.where(abs_diffs == magnitude)[0].tolist()\n",
    "        if indices:\n",
    "            failure_dict[magnitude] = indices\n",
    "            \n",
    "    return failure_dict\n",
    "\n",
    "def run_inference(model: torch.nn.Module, \n",
    "                  dataloader: torch.utils.data.DataLoader, \n",
    "                  device: str) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Runs inference and tracks latency. Returns predictions, true labels, and avg latency per sample (ms).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X).argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    num_samples = len(all_labels)\n",
    "    avg_latency_ms = (total_time / num_samples) * 1000\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), avg_latency_ms\n",
    "\n",
    "def save_test_results(experiment_name: str, \n",
    "                      model: torch.nn.Module, \n",
    "                      test_dataloader: torch.utils.data.DataLoader, \n",
    "                      device: str,\n",
    "                      save_dir: str = \"experiments/results\"):\n",
    "    \"\"\"\n",
    "    Orchestrates the testing process and saves all research-grade metrics.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    preds, labels, latency_ms = run_inference(model, test_dataloader, device)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    class_report = classification_report(labels, preds, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    ordinal_metrics = calculate_ordinal_metrics(preds, labels)\n",
    "    failure_indices = categorize_failures(preds, labels)\n",
    "    \n",
    "    final_metrics = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"global_accuracy\": acc,\n",
    "        \"inference_latency_ms\": latency_ms,\n",
    "        \"ordinal_metrics\": ordinal_metrics,\n",
    "        \"catastrophic_failure_counts\": {k: len(v) for k, v in failure_indices.items()},\n",
    "        \"classification_report\": class_report\n",
    "    }\n",
    "\n",
    "    json_path = f\"{save_dir}/{experiment_name}_metrics.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(final_metrics, f, indent=4)\n",
    "        \n",
    "    npy_path = f\"{save_dir}/{experiment_name}_confusion_matrix.npy\"\n",
    "    np.save(npy_path, conf_matrix)\n",
    "    \n",
    "    # Failure Indices JSON (for later visual analysis of specific FENs)\n",
    "    failures_path = f\"{save_dir}/{experiment_name}_failure_indices.json\"\n",
    "    with open(failures_path, \"w\") as f:\n",
    "        json.dump(failure_indices, f)\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"[Results] Accuracy:        {acc*100:.2f}%\")\n",
    "    print(f\"[Results] Off-by-1 Acc:    {ordinal_metrics['off_by_one_accuracy']*100:.2f}%\")\n",
    "    print(f\"[Results] MAE:             {ordinal_metrics['mae']:.4f}\")\n",
    "    print(f\"[Results] Latency:         {latency_ms:.4f} ms/sample\")\n",
    "    print(\"[Results] Catastrophic Failures (Count):\")\n",
    "    for k in sorted(failure_indices.keys()):\n",
    "        print(f\"   - Off by {k}: {len(failure_indices[k])} samples\")\n",
    "    print(f\"[Results] Saved all metrics to {save_dir}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed637bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"input_shape\": 775,\n",
    "    \"output_shape\": 7\n",
    "}\n",
    "\n",
    "dataset_paths = {\n",
    "    \"train\": TRAIN_PATH,\n",
    "    \"val\":   VAL_PATH,\n",
    "    \"test\":  TEST_PATH\n",
    "}\n",
    "\n",
    "save_config_metadata(experiment_name=RUN_ID,\n",
    "                     model=model,\n",
    "                     hyperparams=hyperparams,\n",
    "                     dataset_paths=dataset_paths)\n",
    "\n",
    "# Save Training Logs (Using the 'result' variable from run_experiment)\n",
    "save_training_logs(experiment_name=RUN_ID, \n",
    "                   results_dict=result)\n",
    "\n",
    "save_test_results(experiment_name=RUN_ID,\n",
    "                  model=model,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbe0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(experiment_name: str, save_dir: str = \"experiments/results\"):\n",
    "    matrix_path = f\"{save_dir}/{experiment_name}_confusion_matrix.npy\"\n",
    "    try:\n",
    "        cm = np.load(matrix_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not find matrix at {matrix_path}\")\n",
    "        return\n",
    "\n",
    "    class_names = [\n",
    "        \"White Win\", \"White Decisive\", \"White Better\", \"Equal\", \n",
    "        \"Black Better\", \"Black Decisive\", \"Black Win\"\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    \n",
    "    plt.title(f\"Confusion Matrix: {experiment_name}\")\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# plot_confusion_matrix(RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = Path(f\"./experiments/logs/{model_save_name}/{RUN_ID}_state_learning_curves.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['train_loss'], label='Train Loss', color='blue')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Val Loss', color='orange')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['train_acc'], label='Train Accuracy', color='blue')\n",
    "plt.plot(df['epoch'], df['val_acc'], label='Val Accuracy', color='orange')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a574f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_fen(vector):\n",
    "    \"\"\"\n",
    "    Reconstructs a FEN string from the 775-dim bitboard vector.\n",
    "    \"\"\"\n",
    "\n",
    "    index_to_piece = {\n",
    "        0: 'P', 1: 'N', 2: 'B', 3: 'R', 4: 'Q', 5: 'K',\n",
    "        6: 'p', 7: 'n', 8: 'b', 9: 'r', 10: 'q', 11: 'k'\n",
    "    }\n",
    "    \n",
    "    board = chess.Board(None) \n",
    "    \n",
    "    for piece_idx in range(12):\n",
    "        for square in range(64):\n",
    "            idx = piece_idx * 64 + square\n",
    "            if vector[idx] == 1:\n",
    "                piece = chess.Piece.from_symbol(index_to_piece[piece_idx])\n",
    "                board.set_piece_at(square, piece)\n",
    "                \n",
    "    board.turn = chess.WHITE if vector[768] == 1 else chess.BLACK\n",
    "\n",
    "    castling_fen = \"\"\n",
    "    if vector[769] == 1: castling_fen += \"K\"\n",
    "    if vector[770] == 1: castling_fen += \"Q\"\n",
    "    if vector[771] == 1: castling_fen += \"k\"\n",
    "    if vector[772] == 1: castling_fen += \"q\"\n",
    "    if castling_fen == \"\": castling_fen = \"-\"\n",
    "    \n",
    "    board.set_castling_fen(castling_fen)\n",
    "    \n",
    "    return board.fen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_saliency_map(model, input_tensor, target_class):\n",
    "    \"\"\"\n",
    "    input_tensor: Shape (1, 19, 8, 8) or (1, 775)\n",
    "    target_class: The class index (0-6) you want to explain (e.g., the WRONG prediction)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Enable Gradient Calculation for the Input \n",
    "    input_tensor.requires_grad_()\n",
    "    \n",
    "    # Forward Pass\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    # Focus on the specific class score we want to explain\n",
    "    # (e.g., \"Why did you predict Class 0?\")\n",
    "    score = output[0, target_class]\n",
    "    \n",
    "    # Backward Pass (Calculate Gradients)\n",
    "    score.backward()\n",
    "    \n",
    "    # Get the Gradients (Sensitivity)\n",
    "    # Shape matches input. e.g. (1, 19, 8, 8)\n",
    "    gradients = input_tensor.grad.data.abs()\n",
    "    \n",
    "    # Collapse Channels to get a single 8x8 Heatmap\n",
    "    # For CNN (19, 8, 8) -> Take max across channels -> (8, 8)\n",
    "    if len(gradients.shape) == 4:\n",
    "        heatmap, _ = torch.max(gradients[0], dim=0)\n",
    "    \n",
    "    # For MLP (1, 775) -> Reshape first 768 bits to (12, 8, 8) then max\n",
    "    else:\n",
    "        # Extract just the board bits (0-768), ignore global bits\n",
    "        board_bits = gradients[0, :768].view(12, 8, 8)\n",
    "        heatmap, _ = torch.max(board_bits, dim=0)\n",
    "        \n",
    "    return heatmap\n",
    "\n",
    "def plot_chess_heatmap(heatmap, fen_str):\n",
    "    \"\"\"Overlay the heatmap on a chess board\"\"\"\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # 'hot' colormap: Black/Red = High Attention, Yellow/White = Low Attention\n",
    "    sns.heatmap(heatmap, cmap='hot', annot=False, cbar=True, square=True)\n",
    "    \n",
    "    plt.title(f\"Model Attention Map\\nFEN: {fen_str[:30]}...\")\n",
    "    plt.gca().invert_yaxis() # Match chess board orientation (Rank 1 at bottom)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6023b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import chess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WideMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_units, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Load Failure Indices\n",
    "failure_path = Path(\"./experiments/results/giraffe_baseline_with_dropout_state/run_2025_12_28_giraffe_baseline_with_dropout_state_failure_indices.json\")\n",
    "with open(failure_path, 'r') as f:\n",
    "    failures = json.load(f)\n",
    "\n",
    "# Pick an index from the \"3\" bin (Errors off by 3 classes)\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "target_error_bin = \"3\"\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "if target_error_bin in failures and failures[target_error_bin]:\n",
    "    target_idx = failures[target_error_bin][400] # Pick the first failed case\n",
    "    print(f\"Analyzing failure at dataset index: {target_idx}\")\n",
    "else:\n",
    "    raise ValueError(f\"No failures found for bin {target_error_bin}\")\n",
    "\n",
    "# Load the Dataset Row\n",
    "data_path = Path(\"./dataset_processed_clean_split_with_state/chess_state_bitboard_test.npz\")\n",
    "data = np.load(data_path)\n",
    "\n",
    "# Extract vector and label\n",
    "input_vector = data['X'][target_idx] \n",
    "true_label = int(data['y'][target_idx])\n",
    "\n",
    "# Reconstruct FEN\n",
    "fen_str = vector_to_fen(input_vector)\n",
    "print(f\"Reconstructed FEN: {fen_str}\")\n",
    "print(f\"True Class Label: {true_label}\")\n",
    "\n",
    "# Load Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = Path(\"./giraffe_baseline_with_dropout_state.pth\")\n",
    "\n",
    "model = WideMLP(input_shape=775, hidden_units=2048, output_shape=7)\n",
    "# Load weights (assuming state_dict was saved)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "except:\n",
    "    # Fallback if full model was saved instead of state_dict\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Generate Heatmap\n",
    "input_tensor = torch.tensor(input_vector, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# We usually want to explain the WRONG prediction (why did it pick that?) or the TRUE label (what did it miss?).\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "print(f\"Model Predicted: {predicted_class} (Error: {abs(predicted_class - true_label)} bins)\")\n",
    "\n",
    "# Visualize attention for the PREDICTED (wrong) class\n",
    "heatmap = generate_saliency_map(model, input_tensor, target_class=predicted_class)\n",
    "plot_chess_heatmap(heatmap.cpu(), fen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "board = chess.Board(fen_str)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0624cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_game_phase_performance(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store results: { piece_count: [correct_predictions, total_samples] }\n",
    "    phase_stats = {i: [0, 0] for i in range(33)} # Max 32 pieces on board\n",
    "    \n",
    "    print(\"Analyzing performance across Game Phases...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X).argmax(dim=1)\n",
    "            \n",
    "            X_cpu = X.cpu().numpy()\n",
    "            y_cpu = y.cpu().numpy()\n",
    "            preds_cpu = preds.cpu().numpy()\n",
    "            \n",
    "            for i in range(len(y_cpu)):\n",
    "                # 1. Count Pieces (Sum of first 768 bits)\n",
    "                # Note: This counts Kings too. \n",
    "                piece_count = int(np.sum(X_cpu[i, :768]))\n",
    "                \n",
    "                # 2. Record Correctness\n",
    "                is_correct = 1 if preds_cpu[i] == y_cpu[i] else 0\n",
    "                \n",
    "                phase_stats[piece_count][0] += is_correct\n",
    "                phase_stats[piece_count][1] += 1\n",
    "                \n",
    "    # --- PROCESS & PLOT ---\n",
    "    \n",
    "    counts = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # We aggregate slightly to avoid noise (e.g., group 3-5 pieces together)\n",
    "    # But for a raw plot, let's just do Piece Count 3 to 32\n",
    "    valid_piece_counts = sorted([k for k, v in phase_stats.items() if v[1] > 50]) # Filter rare counts\n",
    "    \n",
    "    for pc in valid_piece_counts:\n",
    "        correct, total = phase_stats[pc]\n",
    "        acc = correct / total\n",
    "        counts.append(pc)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # PLOT\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(counts, accuracies, marker='o', linewidth=2, color='b')\n",
    "    plt.title(\"Model Accuracy vs. Game Phase (Piece Count)\")\n",
    "    plt.xlabel(\"Number of Pieces on Board (Left = Endgame, Right = Opening)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.gca().invert_xaxis() # Standard Chess Style: Endgames (Low numbers) on the Right\n",
    "    plt.show()\n",
    "    \n",
    "    return counts, accuracies\n",
    "\n",
    "# Run it\n",
    "# counts, accs = analyze_game_phase_performance(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "chessenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
