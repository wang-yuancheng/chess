{
    "experiment_name": "run_2026_01_06_wide_mlp_v2",
    "timestamp": "2026-01-06 13:51:44",
    "model_architecture": {
        "class_name": "PyramidalMLP_v2",
        "total_parameters": 1791039,
        "trainable_parameters": 1791039,
        "input_dim": 775,
        "output_dim": 7,
        "structure_summary": "PyramidalMLP_v2(\n  (network): Sequential(\n    (0): Linear(in_features=775, out_features=1024, bias=True)\n    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=1024, out_features=600, bias=True)\n    (5): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2, inplace=False)\n    (8): Linear(in_features=600, out_features=400, bias=True)\n    (9): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.2, inplace=False)\n    (12): Linear(in_features=400, out_features=256, bias=True)\n    (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): ReLU()\n    (15): Dropout(p=0.2, inplace=False)\n    (16): Linear(in_features=256, out_features=128, bias=True)\n    (17): ReLU()\n    (18): Dropout(p=0.5, inplace=False)\n    (19): Linear(in_features=128, out_features=7, bias=True)\n  )\n)"
    },
    "datasets": {
        "train": "dataset_bitmaps\\bitboard_train.npz",
        "val": "dataset_bitmaps\\bitboard_val.npz",
        "test": "dataset_bitmaps\\bitboard_test.npz"
    },
    "hyperparameters": {
        "epochs": 100,
        "batch_size": 2048,
        "learning_rate": 0.001,
        "optimizer": "AdamW",
        "input_shape": 775,
        "output_shape": 7
    },
    "device": "NVIDIA GeForce RTX 3070"
}