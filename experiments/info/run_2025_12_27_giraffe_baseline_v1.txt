1. You are Massively Overfitting (The "Memorization" Problem)
Training Accuracy: ~99.7% (Your model has memorized the training games perfectly).

Validation Accuracy: ~85.1% (It fails to generalize to new positions).

The Smoking Gun: Look at your Validation Loss in the logs:

Epoch 4: 0.5128 (This was your peak performance).

Epoch 100: 1.2668 (The loss has doubled, meaning the model is getting worse and more confident about its wrong answers).

Why this is expected: Your GiraffeBaseline model (Cell 35) has no Dropout layers. A network with 2048 neurons is huge; without Dropout, it will simply memorize the 768-bit pattern of every board in the training set.

2. Your "High Accuracy" is an Illusion (The "Equal" Trap)
You achieved 83% accuracy, which sounds better than the paper's 70%, but look at your Classification Report:

Class 3 (Equal): Support is 57,672 (57% of your data). Precision/Recall is ~90%.

Class 5 (Black Decisive): Precision is only 69%.

Why this is expected: The model learned that guessing "Equal" is safe. It gets an "A" grade largely because the test is mostly "Equal" questions. Your Macro Avg F1-Score (0.75) is the "real" number you should report, which is much closer to the expected Giraffe performance.

3. Immediate Fixes for Your Next Run
To convert these "Expected Baseline" results into "Research Grade" results:

Add Dropout: In your GiraffeBaseline class, add nn.Dropout(p=0.5) after every ReLU. This will close the gap between your 99% Train and 85% Val.

Trust the Checkpoint: Your code correctly saved the model at Epoch 4 (Val Loss: 0.5128). Do not use the Epoch 100 model for your final evaluation; use the one loaded at the end of the log (Loaded best model weights).

Class Weights: (Optional) To fix the Class 3 imbalance, calculate weights for your loss function so the model cares more about Winning/Losing positions.

Verdict: You have successfully reproduced a "Raw" Baseline. It behaves exactly as a simple MLP on bitmaps should: it overfits quickly and leans on the majority class. You are ready to proceed to the improvements (State Vector, Dropout, CNNs).



While validation accuracy remained stable at ~85%, validation loss degraded from 0.51 to 1.26. This divergence indicates that the model began overfitting by generating overconfident predictions on outliers, maximizing the cross-entropy penalty even as classification margins remained static.