{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a556080-2188-45bf-90de-44c12b252e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Iterator, Optional, Any, Generator, Tuple\n",
    "from enum import Enum\n",
    "import sys, asyncio\n",
    "import chess\n",
    "import chess.pgn\n",
    "import chess.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9204531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stockfish executable: stockfish\\stockfish-windows-x86-64-avx2.exe\n"
     ]
    }
   ],
   "source": [
    "if sys.platform.startswith(\"win\"):\n",
    "    stockfish_executable_path = Path(\"./stockfish/stockfish-windows-x86-64-avx2.exe\")\n",
    "    print(f\"Using Stockfish executable: {stockfish_executable_path}\")\n",
    "\n",
    "if sys.platform.startswith(\"darwin\"):\n",
    "    stockfish_executable_path = Path(\"./stockfish/stockfish-macos-m1-apple-silicon\")\n",
    "    print(f\"Using Stockfish executable: {stockfish_executable_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b96090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_games(pgn_path: Path) -> Iterator[chess.pgn.Game]:\n",
    "    \"\"\"Yield games one by one from a PGN file\"\"\"\n",
    "    \n",
    "    if pgn_path.suffix.lower() != \".pgn\":\n",
    "        raise ValueError(f\"Expected a .pgn file, got: {pgn_path.suffix}\")\n",
    "    \n",
    "    with open(pgn_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(f)\n",
    "            if game is None:\n",
    "                break\n",
    "            yield game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27535a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Policy: WindowsSelectorEventLoopPolicy\n",
      "New Policy: WindowsProactorEventLoopPolicy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Why this cell exists:\n",
    "- python-chess launches Stockfish via asyncio.subprocess_exec.\n",
    "- On Windows, the Selector event loop cannot create subprocesses, it raises NotImplementedError.\n",
    "- Some Jupyter kernels on Windows start with the Selector policy by default.\n",
    "- Switching to WindowsProactorEventLoopPolicy enables subprocess support in this notebook.\n",
    "\n",
    "How to use:\n",
    "- Run this cell once before creating the engine.\n",
    "- On macOS or Linux this does nothing and is safe.\n",
    "\"\"\"\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    print(f\"Initial Policy: {type(asyncio.get_event_loop_policy()).__name__}\")\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    print(f\"New Policy: {type(asyncio.get_event_loop_policy()).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fa9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionLabel(Enum):\n",
    "    WHITE_WINNING = 0\n",
    "    WHITE_DECISIVE = 1\n",
    "    WHITE_BETTER = 2\n",
    "    EQUAL = 3\n",
    "    BLACK_BETTER = 4\n",
    "    BLACK_DECISIVE = 5\n",
    "    BLACK_WINNING = 6\n",
    "\n",
    "class GameStage(Enum):\n",
    "    OPENING = 0\n",
    "    MIDDLEGAME = 1\n",
    "    ENDGAME = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7897121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_result(game: chess.pgn.Game) -> float | None:\n",
    "    \"\"\"\n",
    "    Parses the PGN header result into a float.\n",
    "    Returns None if the game is unfinished or unknown ('*').\n",
    "    \"\"\"\n",
    "    res = game.headers.get(\"Result\", \"*\")\n",
    "    if res == \"1-0\":\n",
    "        return 1.0\n",
    "    elif res == \"0-1\":\n",
    "        return 0.0\n",
    "    elif res == \"1/2-1/2\":\n",
    "        return 0.5\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tapered_phase_score(board: chess.Board) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the game phase based on Non-Pawn Material (NPM).\n",
    "    Returns a phase factor 'p' where:\n",
    "    - 1.0 represents the Start of the game (Opening/Middlegame).\n",
    "    - 0.0 represents a completely empty board (Pure Endgame).\n",
    "    \n",
    "    Credits: Stockfish\n",
    "    \"\"\"\n",
    "    \n",
    "    phase = 0\n",
    "    MAX_PHASE = 24\n",
    "    phase_weights = {\n",
    "        chess.KNIGHT: 1,\n",
    "        chess.BISHOP: 1,\n",
    "        chess.ROOK: 2,\n",
    "        chess.QUEEN: 4\n",
    "    }\n",
    "\n",
    "    for piece_type, weight in phase_weights.items():\n",
    "        count = len(board.pieces(piece_type, chess.WHITE)) + \\\n",
    "                len(board.pieces(piece_type, chess.BLACK))\n",
    "        phase += count * weight\n",
    "    \n",
    "    # Clamp phase to ensure it never exceeds bounds (e.g. unexpected promotions)\n",
    "    phase = min(phase, MAX_PHASE)\n",
    "    \n",
    "    # Normalize (0.0 to 1.0)\n",
    "    return phase / MAX_PHASE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eed1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_game(game: chess.pgn.Game) -> Generator[Dict[str, Any], None, None]:\n",
    "    \"\"\"\n",
    "    Iterates through a single game and yields a dictionary for every position.\n",
    "    Skip the game if result is unknown.\n",
    "    \"\"\"\n",
    "    result = get_game_result(game)\n",
    "    if result is None:\n",
    "        return  \n",
    "    \n",
    "    board = game.board()\n",
    "    \n",
    "    for move in game.mainline_moves():\n",
    "        try:\n",
    "            board.push(move)\n",
    "            board_copy = board.copy()\n",
    "            legal_moves = list(board_copy.legal_moves)\n",
    "\n",
    "            if legal_moves:\n",
    "                # 3. Apply ONE random legal move (includes captures, checks, etc.)\n",
    "                random_move = random.choice(legal_moves)\n",
    "                board_copy.push(random_move)\n",
    "                \n",
    "                # 4. Calculate stats for this new, fictitious position\n",
    "                aug_fen = board_copy.fen()\n",
    "                aug_phase = get_tapered_phase_score(board_copy)\n",
    "                \n",
    "                if aug_phase > 0.66:\n",
    "                    aug_stage = 0 # Opening\n",
    "                elif aug_phase > 0.15: \n",
    "                    aug_stage = 1 # Middlegame\n",
    "                else:\n",
    "                    aug_stage = 2 # Endgame\n",
    "\n",
    "                aug_check = board_copy.is_check()\n",
    "                \n",
    "                yield {\n",
    "                    \"fen\": aug_fen,\n",
    "                    \"game_result\": result, # Kept for schema compatibility, but unreliable now\n",
    "                    \"game_phase\": aug_phase,\n",
    "                    \"game_stage\": aug_stage,\n",
    "                    \"is_check\": aug_check\n",
    "                }\n",
    "\n",
    "        except ValueError:\n",
    "            continue # Skip illegal moves if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89e5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_evaluator(fen_batch: List[str]) -> List[Tuple[Optional[int], Optional[int]]]:\n",
    "    \"\"\"\n",
    "    Worker function: Starts Stockfish, processes a list of FENs, returns scores then quits.\n",
    "    \"\"\"\n",
    "\n",
    "    if sys.platform.startswith(\"win\"):\n",
    "        engine_path = Path(\"./stockfish/stockfish-windows-x86-64-avx2.exe\")\n",
    "    else:\n",
    "        engine_path = Path(\"./stockfish/stockfish-macos-m1-apple-silicon\")\n",
    "        \n",
    "    results = []\n",
    "    engine = None\n",
    "    \n",
    "    try:\n",
    "        engine = chess.engine.SimpleEngine.popen_uci(str(engine_path))\n",
    "                \n",
    "        for fen in fen_batch:\n",
    "            board = chess.Board(fen)\n",
    "            \n",
    "            # Depth 10 (Impulsive)\n",
    "            info_depth_10 = engine.analyse(board, chess.engine.Limit(depth=10))\n",
    "            score_depth_10 = info_depth_10[\"score\"].white().score(mate_score=10000)\n",
    "            \n",
    "            # Depth 20 (Truth)\n",
    "            info_depth_20 = engine.analyse(board, chess.engine.Limit(depth=20))\n",
    "            score_depth_20 = info_depth_20[\"score\"].white().score(mate_score=10000)\n",
    "            \n",
    "            results.append((score_depth_10, score_depth_20))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Worker Error: {e}\")\n",
    "        results = [(None, None)] * len(fen_batch)\n",
    "        \n",
    "    finally:\n",
    "        if engine:\n",
    "            engine.quit()\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0313def2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ryzen 5900X 12 Core\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce3941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_label(cp: float) -> int:\n",
    "    \"\"\"\n",
    "    Classifies centipawn score into an integer label (0-6).\n",
    "    \n",
    "    0: White Winning (>= 500)\n",
    "    1: White Decisive (300 to 499)\n",
    "    2: White Better   (100 to 299)\n",
    "    3: Equal          (-99 to 99)\n",
    "    4: Black Better   (-100 to -299)\n",
    "    5: Black Decisive (-300 to -499)\n",
    "    6: Black Winning  (<= -500)\n",
    "    \"\"\"\n",
    "    if cp >= 500:       return 0\n",
    "    if 500 > cp >= 300: return 1\n",
    "    if 300 > cp >= 100: return 2\n",
    "    if 100 > cp > -100: return 3\n",
    "    if -100 >= cp > -300: return 4\n",
    "    if -300 > cp >= -500: return 5\n",
    "    return 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f296ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import tqdm\n",
    "\n",
    "def run_parallel_evaluation(position_dataset: List[Dict], chunk_size: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits data into chunks, runs parallel evaluation.\n",
    "    Uses a 'temporary' progress bar that vanishes when done.\n",
    "    \"\"\"\n",
    "    num_workers = 10\n",
    "\n",
    "    fens = [d[\"fen\"] for d in position_dataset]    \n",
    "    chunks = [fens[i:i + chunk_size] for i in range(0, len(fens), chunk_size)]\n",
    "    \n",
    "    flat_results = []\n",
    "    \n",
    "    # ThreadPoolExecutor yields values in the exact same order as the inputs were passed\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        all_batch_results = tqdm(executor.map(worker_evaluator, chunks), \n",
    "                                 total=len(chunks), \n",
    "                                 desc=\"Stockfish Eval\", \n",
    "                                 unit=\"chunk\",\n",
    "                                 leave=False) \n",
    "        \n",
    "        for batch_result in all_batch_results:\n",
    "            flat_results.extend(batch_result)\n",
    "\n",
    "    # Merge Scores into Metadata\n",
    "    labeled_data = []\n",
    "    \n",
    "    for i, scores in enumerate(flat_results):\n",
    "        if scores is not None:\n",
    "            row = position_dataset[i]\n",
    "\n",
    "            score_10 = float(scores[0])\n",
    "            score_20 = float(scores[1])\n",
    "            row[\"stockfish_score_depth_10\"] = score_10\n",
    "            row[\"stockfish_score_depth_20\"] = score_20\n",
    "            row[\"stockfish_label_depth_20\"] = get_score_label(score_20)\n",
    "            labeled_data.append(row)\n",
    "            \n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b439e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def get_next_batch_index(existing_parts) -> int:\n",
    "    indices = []\n",
    "    for f in existing_parts:\n",
    "        try:\n",
    "            indices.append(int(f.stem.split(\"_\")[-1]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return (max(indices) + 1) if indices else 0\n",
    "\n",
    "def make_save_path(output_dir: Path, batch_index: int) -> Path:\n",
    "    return output_dir / f\"data_part_{batch_index:04d}.parquet\"\n",
    "\n",
    "def evaluate_and_save_batch(batch, output_dir: Path, batch_index: int, chunk_size:int ) -> int:\n",
    "    evaluated_batch = run_parallel_evaluation(batch, chunk_size)\n",
    "    df = pd.DataFrame(evaluated_batch)\n",
    "    df.to_parquet(make_save_path(output_dir, batch_index), index=False)\n",
    "\n",
    "    # cleanup\n",
    "    del evaluated_batch, df\n",
    "    gc.collect()\n",
    "\n",
    "    return batch_index + 1\n",
    "\n",
    "def run_batch_pipeline(pgn_folder: Path, target_count: int, batch_size: int, output_dir: Path, chunk_size:int):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    seen_fens = set()\n",
    "    current_batch = []\n",
    "\n",
    "    existing_parts = list(output_dir.glob(\"data_part_*.parquet\"))\n",
    "\n",
    "    # Resume\n",
    "    if existing_parts:\n",
    "        print(f\"Found {len(existing_parts)} existing parts. Loading seen FENs to resume...\")\n",
    "        for file_path in tqdm(existing_parts, desc=\"Resuming\"):\n",
    "            try:\n",
    "                df_existing = pd.read_parquet(file_path, columns=[\"fen\"])\n",
    "                seen_fens.update(df_existing[\"fen\"].tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}: {e}\")\n",
    "\n",
    "    batch_index = get_next_batch_index(existing_parts)\n",
    "    total_collected = len(seen_fens)\n",
    "\n",
    "    print(f\"Resumed with {total_collected} positions. Next Batch Index: {batch_index}\")\n",
    "    print(f\"Starting pipeline. Target: {target_count} | Batch Size: {batch_size}\")\n",
    "\n",
    "    pgn_files = list(pgn_folder.glob(\"*.pgn\"))\n",
    "    if not pgn_files:\n",
    "        print(\"No PGN files found!\")\n",
    "        return\n",
    "\n",
    "    pbar = tqdm(total=target_count, initial=total_collected, desc=\"Total Progress\", unit=\"pos\")\n",
    "\n",
    "    for pgn_file in pgn_files:\n",
    "        if total_collected >= target_count:\n",
    "            break\n",
    "\n",
    "        for game in iter_games(pgn_file):\n",
    "            if total_collected >= target_count:\n",
    "                break\n",
    "\n",
    "            for position in process_game(game):\n",
    "                fen = position[\"fen\"]\n",
    "\n",
    "                if fen in seen_fens:\n",
    "                    continue\n",
    "\n",
    "                seen_fens.add(fen)\n",
    "                current_batch.append(position)\n",
    "                total_collected += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                if len(current_batch) >= batch_size:\n",
    "                    batch_index = evaluate_and_save_batch(current_batch, output_dir, batch_index, chunk_size)\n",
    "                    current_batch.clear()\n",
    "\n",
    "                if total_collected >= target_count:\n",
    "                    break\n",
    "\n",
    "    # Final partial batch\n",
    "    if current_batch:\n",
    "        print(f\"Processing final partial batch of {len(current_batch)}...\")\n",
    "        batch_index = evaluate_and_save_batch(current_batch, output_dir, batch_index, chunk_size)\n",
    "        print(f\"Saved final batch {batch_index - 1}\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"Run Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ee491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 existing parts. Loading seen FENs to resume...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resuming: 100%|██████████| 100/100 [00:00<00:00, 128.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed with 1000000 positions. Next Batch Index: 100\n",
      "Starting pipeline. Target: 5000000 | Batch Size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress: 100%|██████████| 5000000/5000000 [72:25:23<00:00, 15.34pos/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete.\n"
     ]
    }
   ],
   "source": [
    "TARGET_POSITIONS = 5000000   \n",
    "BATCH_SIZE = 10000           # Saves 1 file every 10k positions (100 files total)\n",
    "CHUNK_SIZE = 250             # 10,000 / 250 = 40 tasks. Perfect for 12 workers.    \n",
    "\n",
    "GAMES_FOLDER = Path(\"./cclr/train\")\n",
    "OUTPUT_DIR = Path(\"./dataset_parts\")\n",
    "\n",
    "run_batch_pipeline(GAMES_FOLDER, TARGET_POSITIONS, BATCH_SIZE, OUTPUT_DIR, CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d72495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing on 500 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:11<00:00, 42.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "DONE!\n",
      "Original Positions: 5000000\n",
      "Positions Removed:  26698\n",
      "Positions Kept:     4973302\n",
      "Percentage Lost:    0.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"./dataset_parts\")      \n",
    "OUTPUT_DIR = Path(\"./dataset_clean\")   \n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def get_chess_class_vectorized(scores):\n",
    "    conditions = [\n",
    "        (scores >= 500),                     # 0: White Winning\n",
    "        (scores >= 300) & (scores < 500),    # 1: White Decisive\n",
    "        (scores >= 100) & (scores < 300),    # 2: White Better\n",
    "        (scores > -100) & (scores < 100),    # 3: Equal\n",
    "        (scores > -300) & (scores <= -100),  # 4: Black Better\n",
    "        (scores > -500) & (scores <= -300),  # 5: Black Decisive\n",
    "        (scores <= -500)                     # 6: Black Winning\n",
    "    ]\n",
    "    choices = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    return np.select(conditions, choices, default=3)\n",
    "\n",
    "def filter_and_save(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df['calc_label_d10'] = get_chess_class_vectorized(df['stockfish_score_depth_10'])\n",
    "    df['calc_label_d20'] = get_chess_class_vectorized(df['stockfish_score_depth_20'])\n",
    "    \n",
    "    # Filters\n",
    "    stability_mask = abs(df['calc_label_d10'] - df['calc_label_d20']) < 2\n",
    "    is_mate_d20 = abs(df['stockfish_score_depth_20']) > 9000\n",
    "    is_blind_d10 = abs(df['stockfish_score_depth_10']) < 300\n",
    "    mate_trap_mask = (is_mate_d20 & is_blind_d10)\n",
    "    final_mask = stability_mask & (~mate_trap_mask)\n",
    "    \n",
    "    # Apply Filter\n",
    "    original_count = len(df)\n",
    "    df_clean = df[final_mask].copy()\n",
    "    \n",
    "    # Clean up the temporary columns\n",
    "    df_clean.drop(columns=['calc_label_d10', 'calc_label_d20'], inplace=True)\n",
    "    \n",
    "    filename = os.path.basename(file_path)\n",
    "    save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df_clean.to_parquet(save_path, index=False)\n",
    "    \n",
    "    return original_count, len(df_clean)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "parquet_files = glob.glob(os.path.join(INPUT_DIR, \"*.parquet\"))\n",
    "total_kept = 0\n",
    "total_removed = 0\n",
    "\n",
    "print(f\"Starting processing on {len(parquet_files)} files...\")\n",
    "\n",
    "for f in tqdm(parquet_files):\n",
    "    orig, kept = filter_and_save(f)\n",
    "    total_kept += kept\n",
    "    total_removed += (orig - kept)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"DONE!\")\n",
    "print(f\"Original Positions: {total_kept + total_removed}\")\n",
    "print(f\"Positions Removed:  {total_removed}\")\n",
    "print(f\"Positions Kept:     {total_kept}\")\n",
    "print(f\"Percentage Lost:    {total_removed / (total_kept + total_removed) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "With side to move feature\n",
    "Did not swap black to move to white to move\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "INPUT_DIR = Path(\"./dataset_clean\")\n",
    "OUTPUT_DIR = Path(\"./dataset_ready\")\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Define File Indices (Human: 0-99, Computer: 100-499)\n",
    "HUMAN_FILES = sorted(list(INPUT_DIR.glob(\"*part_00[0-9][0-9].parquet\"))) # Matches 0000-0099\n",
    "COMP_FILES = sorted(list(INPUT_DIR.glob(\"*part_0[1-4][0-9][0-9].parquet\"))) # Matches 0100-0499\n",
    "\n",
    "all_files = sorted(list(INPUT_DIR.glob(\"*.parquet\")))\n",
    "human_files_sorted = all_files[0:100]\n",
    "comp_files_sorted = all_files[100:500]\n",
    "\n",
    "train_files = human_files_sorted[0:80] + comp_files_sorted[0:320]\n",
    "val_files = human_files_sorted[80:90] + comp_files_sorted[320:360]\n",
    "test_files = human_files_sorted[90:100] + comp_files_sorted[360:400]\n",
    "\n",
    "print(f\"Split Summary:\")\n",
    "print(f\"Train Files: {len(train_files)}\")\n",
    "print(f\"Val Files:   {len(val_files)}\")\n",
    "print(f\"Test Files:  {len(test_files)}\")\n",
    "\n",
    "def get_mirrored_fen(fen):\n",
    "    \"\"\"\n",
    "     Geometric Flip + Color Inversion.\n",
    "    \"\"\"\n",
    "    board = chess.Board(fen)\n",
    "    return board.mirror().fen()\n",
    "\n",
    "def get_mirrored_label(label):\n",
    "    \"\"\"\n",
    "    Maps White-based labels to Black-based labels.\n",
    "    0 (White Win) <-> 6 (Black Win)\n",
    "    1 (White Dec) <-> 5 (Black Dec)\n",
    "    2 (White Bet) <-> 4 (Black Bet)\n",
    "    3 (Equal)     <-> 3 (Equal)\n",
    "    \"\"\"\n",
    "    mapping = {0: 6, 1: 5, 2: 4, 3: 3, 4: 2, 5: 1, 6: 0}\n",
    "    return mapping.get(label, 3)\n",
    "\n",
    "def load_and_augment(file_list, split_name):\n",
    "    \"\"\"\n",
    "    Loads files, mirrors non-equal classes, and returns a unified DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {split_name} ---\")\n",
    "    \n",
    "    df_list = []\n",
    "    for f in tqdm(file_list, desc=\"Loading Parquet\"):\n",
    "        df_list.append(pd.read_parquet(f))\n",
    "    \n",
    "    full_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Original Count: {len(full_df)}\")\n",
    "    \n",
    "    # Identify rows to mirror\n",
    "    mask_augment = full_df[\"stockfish_label_depth_20\"] != 3\n",
    "    df_to_mirror = full_df[mask_augment].copy()\n",
    "    \n",
    "    if len(df_to_mirror) > 0:\n",
    "        tqdm.pandas(desc=\"Mirroring FENs\")\n",
    "        \n",
    "        df_to_mirror[\"fen\"] = df_to_mirror[\"fen\"].progress_apply(get_mirrored_fen)\n",
    "        \n",
    "        df_to_mirror[\"stockfish_label_depth_20\"] = df_to_mirror[\"stockfish_label_depth_20\"].apply(get_mirrored_label)\n",
    "        \n",
    "        # Invert scores\n",
    "        if \"stockfish_score_depth_20\" in df_to_mirror.columns:\n",
    "             df_to_mirror[\"stockfish_score_depth_20\"] = -df_to_mirror[\"stockfish_score_depth_20\"]\n",
    "        if \"stockfish_score_depth_10\" in df_to_mirror.columns:\n",
    "             df_to_mirror[\"stockfish_score_depth_10\"] = -df_to_mirror[\"stockfish_score_depth_10\"]\n",
    "            \n",
    "        augmented_df = pd.concat([full_df, df_to_mirror], ignore_index=True)\n",
    "    else:\n",
    "        augmented_df = full_df\n",
    "        \n",
    "    print(f\"Post-Augmentation Count: {len(augmented_df)}\")\n",
    "    return augmented_df\n",
    "\n",
    "def balance_classes(df):\n",
    "    \"\"\"\n",
    "    Downsamples all classes to match the count of the minority class.\n",
    "    \"\"\"\n",
    "    groups = df.groupby(\"stockfish_label_depth_20\")\n",
    "    counts = groups.size()\n",
    "    \n",
    "    print(\"\\nClass Distribution (Before Balancing):\")\n",
    "    print(counts)\n",
    "    \n",
    "    min_count = counts.min()\n",
    "    print(f\"\\nTarget Samples per Class: {min_count}\")\n",
    "    \n",
    "    # Stratified Sampling\n",
    "    balanced_df = groups.sample(n=min_count, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Class Distribution (After Balancing):\")\n",
    "    print(balanced_df[\"stockfish_label_depth_20\"].value_counts())\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (OUTPUT_DIR / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "splits = [\n",
    "    (\"train\", train_files),\n",
    "    (\"val\", val_files),\n",
    "    (\"test\", test_files)\n",
    "]\n",
    "\n",
    "for split_name, files in splits:\n",
    "    df_aug = load_and_augment(files, split_name)\n",
    "    df_clean = balance_classes(df_aug)\n",
    "\n",
    "    save_path = OUTPUT_DIR / split_name / f\"{split_name}_balanced.parquet\"\n",
    "    print(f\"Saving to {save_path}...\")\n",
    "    df_clean.to_parquet(save_path, index=False)\n",
    "    \n",
    "    del df_aug, df_clean\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nProcessing Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba42f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Summary:\n",
      "Train Files: 400\n",
      "Val Files:   50\n",
      "Test Files:  50\n",
      "\n",
      "--- Processing train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Parquet: 100%|██████████| 400/400 [00:05<00:00, 69.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Count: 3978504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Canonical Flipping: 100%|██████████| 1990686/1990686 [02:46<00:00, 11942.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Flip Count: 3978504 (Count stays identical)\n",
      "\n",
      "Class Distribution (Before Balancing):\n",
      "stockfish_label_depth_20\n",
      "0    1085185\n",
      "1     568624\n",
      "2     479232\n",
      "3    1386704\n",
      "4     178914\n",
      "5     125957\n",
      "6     153888\n",
      "dtype: int64\n",
      "\n",
      "Target Samples per Class: 125957\n",
      "Class Distribution (After Balancing):\n",
      "stockfish_label_depth_20\n",
      "0    125957\n",
      "1    125957\n",
      "2    125957\n",
      "3    125957\n",
      "4    125957\n",
      "5    125957\n",
      "6    125957\n",
      "Name: count, dtype: int64\n",
      "Saving to dataset_ready\\train\\train_balanced.parquet...\n",
      "\n",
      "--- Processing val ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Parquet: 100%|██████████| 50/50 [00:00<00:00, 69.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Count: 497423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Canonical Flipping: 100%|██████████| 248764/248764 [00:20<00:00, 12208.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Flip Count: 497423 (Count stays identical)\n",
      "\n",
      "Class Distribution (Before Balancing):\n",
      "stockfish_label_depth_20\n",
      "0    129343\n",
      "1     71917\n",
      "2     62732\n",
      "3    178697\n",
      "4     22869\n",
      "5     15328\n",
      "6     16537\n",
      "dtype: int64\n",
      "\n",
      "Target Samples per Class: 15328\n",
      "Class Distribution (After Balancing):\n",
      "stockfish_label_depth_20\n",
      "0    15328\n",
      "1    15328\n",
      "2    15328\n",
      "3    15328\n",
      "4    15328\n",
      "5    15328\n",
      "6    15328\n",
      "Name: count, dtype: int64\n",
      "Saving to dataset_ready\\val\\val_balanced.parquet...\n",
      "\n",
      "--- Processing test ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Parquet: 100%|██████████| 50/50 [00:00<00:00, 78.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Count: 497375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Canonical Flipping: 100%|██████████| 248699/248699 [00:20<00:00, 12327.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Flip Count: 497375 (Count stays identical)\n",
      "\n",
      "Class Distribution (Before Balancing):\n",
      "stockfish_label_depth_20\n",
      "0    132519\n",
      "1     71869\n",
      "2     61043\n",
      "3    170682\n",
      "4     23678\n",
      "5     16961\n",
      "6     20623\n",
      "dtype: int64\n",
      "\n",
      "Target Samples per Class: 16961\n",
      "Class Distribution (After Balancing):\n",
      "stockfish_label_depth_20\n",
      "0    16961\n",
      "1    16961\n",
      "2    16961\n",
      "3    16961\n",
      "4    16961\n",
      "5    16961\n",
      "6    16961\n",
      "Name: count, dtype: int64\n",
      "Saving to dataset_ready\\test\\test_balanced.parquet...\n",
      "\n",
      "Processing Complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Without side to move feature\n",
    "Swapped black to move to white to move\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "INPUT_DIR = Path(\"./dataset_clean\")\n",
    "OUTPUT_DIR = Path(\"./dataset_ready\")\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Define File Indices (Human: 0-99, Computer: 100-499)\n",
    "HUMAN_FILES = sorted(list(INPUT_DIR.glob(\"*part_00[0-9][0-9].parquet\"))) # Matches 0000-0099\n",
    "COMP_FILES = sorted(list(INPUT_DIR.glob(\"*part_0[1-4][0-9][0-9].parquet\"))) # Matches 0100-0499\n",
    "\n",
    "all_files = sorted(list(INPUT_DIR.glob(\"*.parquet\")))\n",
    "human_files_sorted = all_files[0:100]\n",
    "comp_files_sorted = all_files[100:500]\n",
    "\n",
    "train_files = human_files_sorted[0:80] + comp_files_sorted[0:320]\n",
    "val_files = human_files_sorted[80:90] + comp_files_sorted[320:360]\n",
    "test_files = human_files_sorted[90:100] + comp_files_sorted[360:400]\n",
    "\n",
    "print(f\"Split Summary:\")\n",
    "print(f\"Train Files: {len(train_files)}\")\n",
    "print(f\"Val Files:   {len(val_files)}\")\n",
    "print(f\"Test Files:  {len(test_files)}\")\n",
    "\n",
    "def get_mirrored_fen(fen):\n",
    "    \"\"\"\n",
    "     Geometric Flip + Color Inversion.\n",
    "    \"\"\"\n",
    "    board = chess.Board(fen)\n",
    "    return board.mirror().fen()\n",
    "\n",
    "def get_mirrored_label(label):\n",
    "    \"\"\"\n",
    "    Maps White-based labels to Black-based labels.\n",
    "    0 (White Win) <-> 6 (Black Win)\n",
    "    1 (White Dec) <-> 5 (Black Dec)\n",
    "    2 (White Bet) <-> 4 (Black Bet)\n",
    "    3 (Equal)     <-> 3 (Equal)\n",
    "    \"\"\"\n",
    "    mapping = {0: 6, 1: 5, 2: 4, 3: 3, 4: 2, 5: 1, 6: 0}\n",
    "    return mapping.get(label, 3)\n",
    "\n",
    "def load_and_augment(file_list, split_name):\n",
    "    \"\"\"\n",
    "    Loads files, FLIPS all Black-to-move positions to White-to-move (Canonical),\n",
    "    and returns a unified DataFrame WITHOUT creating duplicates.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {split_name} ---\")\n",
    "    \n",
    "    df_list = []\n",
    "    for f in tqdm(file_list, desc=\"Loading Parquet\"):\n",
    "        df_list.append(pd.read_parquet(f))\n",
    "    \n",
    "    full_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Original Count: {len(full_df)}\")\n",
    "    \n",
    "    # Identify Black-to-move rows\n",
    "    mask_black = full_df[\"fen\"].str.contains(\" b \")\n",
    "\n",
    "    tqdm.pandas(desc=\"Canonical Flipping\")\n",
    "\n",
    "    # Flip Board\n",
    "    # We apply get_mirrored_fen ONLY to the Black rows. \n",
    "    # This converts them to \"White to Move\" with pieces mirrored.\n",
    "    full_df.loc[mask_black, \"fen\"] = full_df.loc[mask_black, \"fen\"].progress_apply(get_mirrored_fen)\n",
    "    \n",
    "    # Flip Label (6->0, 5->1, etc.)\n",
    "    full_df.loc[mask_black, \"stockfish_label_depth_20\"] = full_df.loc[mask_black, \"stockfish_label_depth_20\"].apply(get_mirrored_label)\n",
    "\n",
    "    # Flip Score\n",
    "    if \"stockfish_score_depth_20\" in full_df.columns:\n",
    "        full_df.loc[mask_black, \"stockfish_score_depth_20\"] = -full_df.loc[mask_black, \"stockfish_score_depth_20\"]\n",
    "    if \"stockfish_score_depth_10\" in full_df.columns:\n",
    "        full_df.loc[mask_black, \"stockfish_score_depth_10\"] = -full_df.loc[mask_black, \"stockfish_score_depth_10\"]\n",
    "\n",
    "    print(f\"Post-Flip Count: {len(full_df)} (Count stays identical)\")\n",
    "    return full_df\n",
    "\n",
    "def balance_classes(df):\n",
    "    \"\"\"\n",
    "    Downsamples all classes to match the count of the minority class.\n",
    "    \"\"\"\n",
    "    groups = df.groupby(\"stockfish_label_depth_20\")\n",
    "    counts = groups.size()\n",
    "    \n",
    "    print(\"\\nClass Distribution (Before Balancing):\")\n",
    "    print(counts)\n",
    "    \n",
    "    min_count = counts.min()\n",
    "    print(f\"\\nTarget Samples per Class: {min_count}\")\n",
    "    \n",
    "    # Stratified Sampling\n",
    "    balanced_df = groups.sample(n=min_count, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Class Distribution (After Balancing):\")\n",
    "    print(balanced_df[\"stockfish_label_depth_20\"].value_counts())\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (OUTPUT_DIR / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "splits = [\n",
    "    (\"train\", train_files),\n",
    "    (\"val\", val_files),\n",
    "    (\"test\", test_files)\n",
    "]\n",
    "\n",
    "for split_name, files in splits:\n",
    "    df_aug = load_and_augment(files, split_name)\n",
    "    df_clean = balance_classes(df_aug)\n",
    "\n",
    "    save_path = OUTPUT_DIR / split_name / f\"{split_name}_balanced.parquet\"\n",
    "    print(f\"Saving to {save_path}...\")\n",
    "    df_clean.to_parquet(save_path, index=False)\n",
    "    \n",
    "    del df_aug, df_clean\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nProcessing Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e5234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "chessenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
