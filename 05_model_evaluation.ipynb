{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e383912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chess\n",
    "import chess.svg\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from IPython.display import display, SVG\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 2048 \n",
    "TEST_PATH = Path(\"./dataset_bitmaps/bitboard_test.npz\") \n",
    "RESULTS_DIR = Path(\"experiments/results\")\n",
    "LOGS_DIR = Path(\"experiments/logs\")\n",
    "\n",
    "# ==========================================\n",
    "MODEL_NAME = \"wide_mlp_v1\"\n",
    "MODEL_TYPE = \"WideMLP\"\n",
    "MODEL_PATH = \"wide_mlp_v1.pth\"\n",
    "INPUT_SHAPE = 775\n",
    "HIDDEN_UNITS = 2048 \n",
    "OUTPUT_SHAPE = 7\n",
    "FAILURE_INDICES_PATH = RESULTS_DIR / MODEL_NAME /\"run_2026_01_05_wide_mlp_v1_failure_indices.json\"   \n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=hidden_units),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_units, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class PyramidalMLP_v2(nn.Module):\n",
    "    def __init__(self, input_shape=775, output_shape=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_shape, 1024), \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(1024, 600),\n",
    "            nn.BatchNorm1d(600),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(600, 400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(400, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(128, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4682b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from wide_mlp_v1.pth\n",
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "class NPZChessDataset(Dataset):\n",
    "    def __init__(self, npz_path: Path):\n",
    "        with np.load(npz_path) as data:\n",
    "            self.X = torch.tensor(data[\"X\"], dtype=torch.float32)\n",
    "            self.y = torch.tensor(data[\"y\"], dtype=torch.long)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "def get_test_loader():\n",
    "    dataset = NPZChessDataset(TEST_PATH)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ==========================================\n",
    "if MODEL_TYPE == \"WideMLP\":\n",
    "    model = WideMLP(INPUT_SHAPE, HIDDEN_UNITS, OUTPUT_SHAPE).to(DEVICE)\n",
    "elif MODEL_TYPE == \"PyramidalMLP\":\n",
    "    model = PyramidalMLP_v2(INPUT_SHAPE, OUTPUT_SHAPE).to(DEVICE)\n",
    "# ==========================================\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    print(f\"Loaded weights from {MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Weight file not found! Please check MODEL_PATH.\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cec762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map(model, input_tensor, target_class):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the score for the target class w.r.t the input vector.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_tensor.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    pred = model(input_tensor)\n",
    "    score = pred[0, target_class]\n",
    "    \n",
    "    # Backward pass\n",
    "    score.backward()\n",
    "    \n",
    "    # Get gradients\n",
    "    gradients = input_tensor.grad.data.cpu().numpy()[0]\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def plot_chess_heatmap(gradients, fen_str, true_label, pred_label):\n",
    "    \"\"\"\n",
    "    Aggregates gradients from the 12 bitboards (first 768 indices) into a 8x8 grid.\n",
    "    \"\"\"\n",
    "    # Reshape first 768 features into (12 channels, 64 squares)\n",
    "    # The order of channels : P, N, B, R, Q, K, p, n, b, r, q, k\n",
    "    piece_grads = gradients[:768].reshape(12, 64)\n",
    "    \n",
    "    # Sum absolute gradients across all piece channels to get importance per square\n",
    "    # Shape becomes (64,)\n",
    "    saliency = np.sum(np.abs(piece_grads), axis=0)\n",
    "    \n",
    "    # Reshape into 8x8 board\n",
    "    heatmap = saliency.reshape(8, 8)\n",
    "    \n",
    "    # Flip logic: rank 1 is usually index 0-7, rank 8 is 56-63.\n",
    "    # Matplotlib plots 0 at top-left by default, but chess rank 8 is top.\n",
    "    # We usually just need to reshape correctly.\n",
    "    # Let's assume standard: Index 0 is a1 (bottom-left).\n",
    "    # We need to flip vertically for imshow to put Rank 8 at top.\n",
    "    heatmap = np.flipud(heatmap) \n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.heatmap(heatmap, cmap=\"viridis\", alpha=0.7, zorder=2, ax=ax, cbar=False)\n",
    "    \n",
    "    # Load Board Image if FEN provided\n",
    "    if fen_str:\n",
    "        # Save svg to temp file or render on top? \n",
    "        # Easier approach: Use text over heatmap or side-by-side\n",
    "        ax.set_title(f\"Saliency Map (True: {true_label}, Pred: {pred_label})\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display actual board below\n",
    "        print(\"Actual Board Position:\")\n",
    "        display(chess.svg.board(chess.Board(fen_str), size=350))\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def vector_to_fen(vector):\n",
    "    \"\"\"\n",
    "    Reconstructs a FEN string from the 775-dim bitboard vector.\n",
    "    \"\"\"\n",
    "\n",
    "    index_to_piece = {\n",
    "        0: 'P', 1: 'N', 2: 'B', 3: 'R', 4: 'Q', 5: 'K',\n",
    "        6: 'p', 7: 'n', 8: 'b', 9: 'r', 10: 'q', 11: 'k'\n",
    "    }\n",
    "    \n",
    "    board = chess.Board(None) \n",
    "    \n",
    "    for piece_idx in range(12):\n",
    "        for square in range(64):\n",
    "            idx = piece_idx * 64 + square\n",
    "            if vector[idx] == 1:\n",
    "                piece = chess.Piece.from_symbol(index_to_piece[piece_idx])\n",
    "                board.set_piece_at(square, piece)\n",
    "                \n",
    "    board.turn = chess.WHITE if vector[768] == 1 else chess.BLACK\n",
    "\n",
    "    castling_fen = \"\"\n",
    "    if vector[769] == 1: castling_fen += \"K\"\n",
    "    if vector[770] == 1: castling_fen += \"Q\"\n",
    "    if vector[771] == 1: castling_fen += \"k\"\n",
    "    if vector[772] == 1: castling_fen += \"q\"\n",
    "    if castling_fen == \"\": castling_fen = \"-\"\n",
    "    \n",
    "    board.set_castling_fen(castling_fen)\n",
    "    \n",
    "    return board.fen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.33430448e-03 -4.13293578e-03  5.56480885e-03  1.18819419e-02\n",
      "  7.69415498e-03  1.03726494e-03  3.68698314e-03  1.63372443e-03\n",
      " -1.92809328e-01  4.83172089e-02 -3.46954525e-01  1.25636905e-01\n",
      " -8.77027273e-01 -5.59846818e-01 -3.79317999e-01 -2.21662521e-02\n",
      "  7.05590770e-02  3.75913471e-01  2.97038198e-01 -5.80576956e-02\n",
      " -2.35117644e-01 -5.60871661e-01 -3.97791743e-01 -2.16712549e-01\n",
      "  1.05204411e-01 -1.92649662e-04  5.52715778e-01  1.25195235e-02\n",
      " -1.13929912e-01 -8.12322497e-01 -1.29273385e-02 -7.43571520e-02\n",
      "  9.18054208e-02 -6.78124309e-01 -1.10529214e-02 -1.95739940e-01\n",
      " -2.20500350e-01 -9.61963534e-02 -8.89158845e-02  1.33558095e-01\n",
      " -6.96933150e-01 -8.65940690e-01 -2.07026213e-01  1.81053713e-01\n",
      " -4.76596415e-01 -9.33789611e-02  1.24928236e-01  2.81348228e-01\n",
      " -1.37934184e+00 -1.81829214e+00 -5.98194003e-01 -5.54945469e-01\n",
      " -1.17258024e+00 -9.92037177e-01  4.20202821e-01 -2.96234190e-01\n",
      "  4.99806646e-03  4.82312031e-03 -2.48831068e-03  9.61325038e-03\n",
      "  9.80768818e-04  3.02116340e-03 -1.76426861e-03  1.83864473e-03\n",
      " -1.28573871e+00 -1.33234370e+00 -1.93245387e+00 -8.17760348e-01\n",
      " -2.70600128e+00 -2.12369108e+00 -2.18880939e+00 -1.23070300e+00\n",
      " -8.09779406e-01 -1.43800998e+00 -2.58783650e+00 -2.06114984e+00\n",
      " -2.31575513e+00 -1.62930393e+00 -2.31721950e+00 -2.36635208e+00\n",
      " -1.43368411e+00 -1.70188189e+00 -2.19478941e+00 -2.04052973e+00\n",
      " -2.72915697e+00 -2.72630429e+00 -2.99981785e+00 -2.65294218e+00\n",
      " -7.55068421e-01 -1.20238078e+00 -1.97115493e+00 -2.58626056e+00\n",
      " -1.78545427e+00 -2.95519137e+00 -2.42475319e+00 -2.11350822e+00\n",
      " -1.14886856e+00 -1.63212931e+00 -1.78398025e+00 -1.96910596e+00\n",
      " -2.07297421e+00 -2.22936487e+00 -1.48742175e+00 -2.02651381e+00\n",
      " -1.04510903e+00 -1.47408938e+00 -8.57353985e-01 -1.59399939e+00\n",
      " -1.22416854e+00 -2.02932501e+00 -1.31413817e+00 -1.99063969e+00\n",
      " -1.14507771e+00 -1.27949286e+00 -1.55946469e+00 -1.49731243e+00\n",
      " -1.54010355e+00 -2.13716030e+00 -1.76072598e+00 -1.80628693e+00\n",
      " -8.47052336e-01 -5.60210943e-02 -1.19474304e+00 -1.99445724e+00\n",
      " -6.97199106e-02 -1.63651061e+00 -1.42678332e+00 -6.87582731e-01\n",
      " -8.83917153e-01 -1.51313913e+00 -2.13703966e+00 -2.63321590e+00\n",
      " -2.68635392e+00 -2.36432886e+00 -1.76209342e+00 -1.51766777e+00\n",
      " -1.78574038e+00 -1.43766463e+00 -1.88006926e+00 -2.52102447e+00\n",
      " -2.62785387e+00 -2.24681950e+00 -2.04959679e+00 -1.74391246e+00\n",
      " -1.76950479e+00 -2.14534855e+00 -2.45891190e+00 -2.65054560e+00\n",
      " -2.59593964e+00 -2.83549929e+00 -2.60119748e+00 -1.43398356e+00\n",
      " -1.29043400e+00 -2.21344185e+00 -2.32102895e+00 -2.18948364e+00\n",
      " -2.24120045e+00 -2.07582140e+00 -2.62944818e+00 -2.24042201e+00\n",
      " -1.58291161e+00 -2.65709829e+00 -2.57773924e+00 -1.59055185e+00\n",
      " -2.26246953e+00 -1.84901679e+00 -1.39167213e+00 -2.15541482e+00\n",
      " -1.64264178e+00 -1.80059564e+00 -1.12899685e+00 -1.92896676e+00\n",
      " -1.82342172e+00 -1.52691102e+00 -1.64273584e+00 -1.80590224e+00\n",
      " -2.01419926e+00 -1.44488895e+00 -1.58771753e+00 -1.76618600e+00\n",
      " -9.99838650e-01 -1.53067732e+00 -1.54066730e+00 -1.85026896e+00\n",
      " -7.94083118e-01 -1.63345504e+00 -2.07625484e+00 -1.27059484e+00\n",
      " -3.08007061e-01 -1.22891104e+00 -1.40129828e+00 -6.57480478e-01\n",
      " -2.22450185e+00 -2.25497103e+00 -2.58385777e+00 -2.49190617e+00\n",
      " -3.06329632e+00 -2.70129204e+00 -2.16007042e+00 -1.94066882e+00\n",
      " -1.76405239e+00 -2.73784804e+00 -2.75740528e+00 -2.53298283e+00\n",
      " -3.00893950e+00 -2.58878684e+00 -2.39210439e+00 -1.83036625e+00\n",
      " -2.24124980e+00 -2.56444311e+00 -2.91679835e+00 -1.97379184e+00\n",
      " -3.53484774e+00 -3.02480006e+00 -2.70600724e+00 -2.68707848e+00\n",
      " -1.41142547e+00 -2.14883924e+00 -2.30905199e+00 -2.06676579e+00\n",
      " -3.30903053e+00 -2.49220896e+00 -2.64678001e+00 -2.29838991e+00\n",
      " -1.56240952e+00 -1.53352261e+00 -2.07872057e+00 -2.00570488e+00\n",
      " -3.15266824e+00 -1.97499394e+00 -1.62193012e+00 -1.78613663e+00\n",
      " -1.56599116e+00 -1.73868906e+00 -1.51113749e+00 -2.12907743e+00\n",
      " -2.49714804e+00 -2.10122156e+00 -2.27956343e+00 -1.88294220e+00\n",
      " -1.57870376e+00 -1.79796338e+00 -2.24488258e+00 -2.12892628e+00\n",
      " -2.71564770e+00 -1.89609730e+00 -3.17155933e+00 -1.94850016e+00\n",
      " -1.51913786e+00 -1.14889956e+00 -2.26174784e+00 -2.05268502e+00\n",
      " -2.19224453e+00 -2.59175897e+00 -1.64603233e+00 -2.06061029e+00\n",
      " -3.79995298e+00 -3.70550990e+00 -4.03979206e+00 -3.99618006e+00\n",
      " -3.57315159e+00 -4.36121559e+00 -2.89341474e+00 -3.19011021e+00\n",
      " -2.82839012e+00 -3.15637088e+00 -4.15402031e+00 -3.61180997e+00\n",
      " -4.31353855e+00 -3.33095765e+00 -2.87694836e+00 -3.03993559e+00\n",
      " -3.63252807e+00 -3.99481344e+00 -2.79493666e+00 -3.84246469e+00\n",
      " -4.52187395e+00 -4.44323683e+00 -2.85386992e+00 -4.08337641e+00\n",
      " -3.43948770e+00 -2.62115049e+00 -1.97165370e+00 -2.77928543e+00\n",
      " -4.51162434e+00 -3.62511706e+00 -3.28266001e+00 -3.34599400e+00\n",
      " -3.08226585e+00 -2.66329384e+00 -2.64993191e+00 -2.70618320e+00\n",
      " -3.56394148e+00 -2.16177106e+00 -3.17633653e+00 -3.48304987e+00\n",
      " -3.14089584e+00 -3.33462238e+00 -2.35163689e+00 -2.72899628e+00\n",
      " -2.54168105e+00 -2.03619504e+00 -2.14497304e+00 -2.01119184e+00\n",
      " -2.59160161e+00 -2.57175159e+00 -2.89412785e+00 -2.47505021e+00\n",
      " -3.19291735e+00 -1.12582648e+00 -1.99562871e+00 -2.74653578e+00\n",
      " -2.01002002e+00 -2.26629400e+00 -1.97360277e+00 -2.40773630e+00\n",
      " -2.79051018e+00 -9.79351938e-01  2.14814246e-02 -1.54899955e+00\n",
      "  9.54406142e-01 -3.22064161e-02  3.29858139e-02 -1.56323004e+00\n",
      " -1.65390563e+00 -1.80996239e+00 -6.14497900e-01 -6.34926856e-01\n",
      " -7.51268119e-02  3.93749982e-01  2.95637578e-01 -1.78961337e+00\n",
      " -2.09016728e+00 -1.88188303e+00 -9.32532549e-01 -1.75494134e-01\n",
      " -1.47950873e-01 -1.08655252e-01  8.89139622e-02 -1.45014381e+00\n",
      " -1.28126907e+00 -1.05856991e+00 -6.20658159e-01 -5.54344654e-02\n",
      "  3.53333592e-01  2.79691398e-01  3.98700804e-01 -5.41500986e-01\n",
      " -6.37854636e-01 -6.37634575e-01  9.90415812e-02  4.69195366e-01\n",
      "  3.51423144e-01  3.67216229e-01  2.68380225e-01  5.16351700e-01\n",
      "  9.49438065e-02  1.10664070e-02  2.30772138e-01 -9.37331438e-01\n",
      "  5.52438319e-01  7.65035272e-01  8.98819149e-01  3.99312526e-02\n",
      " -1.50450319e-01  4.23208922e-01  2.38526136e-01 -8.06247294e-02\n",
      "  6.81041718e-01  2.60816514e-01  7.02471197e-01  6.83315098e-02\n",
      "  1.52081347e+00  6.34616673e-01  1.12759918e-01  6.24238014e-01\n",
      "  5.00290751e-01  7.43065417e-01  5.57447970e-01  1.17682457e+00\n",
      "  1.12707698e+00  6.21827483e-01  5.45508027e-01  2.49415606e-01\n",
      "  8.45045224e-03  2.11556815e-03  2.55034189e-03  3.56984884e-03\n",
      "  1.56544289e-03 -3.05366470e-03 -1.20027829e-03 -8.23691720e-04\n",
      "  1.18248606e+00  3.02846551e-01  2.00366187e+00  2.04632640e+00\n",
      "  1.98806679e+00  2.38745975e+00  1.11562383e+00  2.40463018e+00\n",
      "  7.37114668e-01  1.00190902e+00  1.50337934e+00  2.32064080e+00\n",
      "  1.70742559e+00  7.64009774e-01  6.98885918e-01  5.66260993e-01\n",
      "  1.47520506e+00  3.66703421e-01  1.16393280e+00  1.56366599e+00\n",
      "  7.99430370e-01  5.77112973e-01  1.75379813e-01 -2.04061583e-01\n",
      "  4.65192497e-01  2.41939291e-01  1.99859068e-01  1.29352283e+00\n",
      " -1.30088046e-01  3.54962587e-01  6.37065887e-01 -5.46371937e-02\n",
      "  2.79185712e-01  5.31409860e-01  3.77402842e-01  6.75096959e-02\n",
      "  2.03908741e-01  6.88759238e-02  5.47714949e-01  7.18166530e-01\n",
      "  7.83312678e-01  4.55246478e-01  4.20157313e-01  4.30697888e-01\n",
      "  4.71062362e-02  3.03194880e-01  5.65391004e-01  5.27772494e-02\n",
      "  1.07543543e-04  1.22148450e-03 -4.34792601e-03  9.66598047e-04\n",
      "  5.54878823e-03 -2.25672219e-03 -2.26099999e-03  7.33278866e-04\n",
      "  1.01870048e+00  1.58114731e+00  2.67802525e+00  1.26273417e+00\n",
      "  9.95216489e-01  6.22462153e-01  1.26613533e+00  9.14951563e-01\n",
      " -2.02947259e-01  6.19578004e-01  6.39604867e-01  2.43316197e+00\n",
      "  8.71924162e-01  2.37103605e+00  7.83468783e-01  1.26840413e+00\n",
      "  1.07060909e+00  1.17125821e+00  1.45401978e+00  1.21144462e+00\n",
      "  1.51088953e+00  1.30298829e+00  1.44366157e+00  1.33237350e+00\n",
      "  8.42886925e-01  1.34380496e+00  8.39148641e-01  2.54131889e+00\n",
      "  1.33206129e+00  1.65043771e+00  1.92254496e+00  1.20386457e+00\n",
      "  1.77188730e+00  7.61011541e-01  1.64693570e+00  1.51596284e+00\n",
      "  1.26416755e+00  1.05425441e+00  1.88391948e+00  1.34242690e+00\n",
      "  4.38736439e-01  1.01336110e+00  1.62223268e+00  1.21731973e+00\n",
      "  1.46777654e+00  1.56881773e+00  1.01635814e+00  9.81434643e-01\n",
      "  6.10152125e-01  5.11654019e-02  9.63371634e-01  1.30480647e+00\n",
      "  9.56015229e-01  9.55062270e-01  5.34421504e-01  1.28118300e+00\n",
      "  1.50043219e-01  1.06289554e+00  1.46547830e+00  9.47796285e-01\n",
      "  1.37629342e+00  1.02082908e+00  1.09593749e+00  1.09554076e+00\n",
      "  4.28373784e-01 -7.62607157e-02  1.04925656e+00  7.20852196e-01\n",
      "  4.44928557e-01  1.26453495e+00  9.64516640e-01  2.64535999e+00\n",
      "  1.52813280e+00  4.03234303e-01  1.43900108e+00  1.15151358e+00\n",
      "  3.92150372e-01  1.23695397e+00  1.64680636e+00  1.13803053e+00\n",
      "  2.52630234e-01  7.29866505e-01  1.06575072e+00  2.17731380e+00\n",
      "  1.08786798e+00  1.31081760e+00  8.01973104e-01  8.81517708e-01\n",
      "  1.23778033e+00  1.19747877e+00  1.70258832e+00  1.91323996e+00\n",
      "  1.15117550e+00  1.12518716e+00  1.47341633e+00  1.03427923e+00\n",
      "  9.59446728e-01  1.64263284e+00  1.19279027e+00  2.59168887e+00\n",
      "  8.85058761e-01  1.13740540e+00  1.29513860e+00  1.54095912e+00\n",
      "  2.20673943e+00  1.46687579e+00  1.55341446e+00  1.41504562e+00\n",
      "  2.37595415e+00  1.28173184e+00  1.52964973e+00  1.20993316e+00\n",
      "  2.10403705e+00  1.37358677e+00  1.00099969e+00  1.32015800e+00\n",
      "  1.07898343e+00  2.38189316e+00  1.61798120e+00  1.28733540e+00\n",
      "  1.36576164e+00  1.36075711e+00  9.90513265e-01  1.26797986e+00\n",
      "  1.33895206e+00  1.50099015e+00  2.11625719e+00  1.15963078e+00\n",
      "  1.88765001e+00  2.12357855e+00  1.95177805e+00  1.83664393e+00\n",
      "  2.22230339e+00  1.91052651e+00  2.41501331e+00  1.85993946e+00\n",
      "  1.78209293e+00  2.09865332e+00  1.89590836e+00  1.80406594e+00\n",
      "  1.84343123e+00  2.86298990e+00  1.69706726e+00  1.51607358e+00\n",
      "  1.55436206e+00  1.54179454e+00  1.71906519e+00  2.16250038e+00\n",
      "  2.48437691e+00  2.20726633e+00  1.18249202e+00  2.30727649e+00\n",
      "  1.65890455e-01  1.75207901e+00  1.21063256e+00  1.74895167e+00\n",
      "  1.62744212e+00  2.35516071e+00  2.49026537e+00  1.30803549e+00\n",
      "  7.30891466e-01  1.53369248e+00  1.49183881e+00  1.15149415e+00\n",
      "  2.33962345e+00  1.68694687e+00  1.85604572e+00  2.22842836e+00\n",
      "  1.42936039e+00  1.55086660e+00  1.67212987e+00  1.45208108e+00\n",
      "  2.41058731e+00  2.04415154e+00  2.33782434e+00  1.40633726e+00\n",
      "  1.47579980e+00  1.61974990e+00  1.31061935e+00  1.69790316e+00\n",
      "  2.13251352e+00  1.82994747e+00  2.14223599e+00  1.30641007e+00\n",
      "  1.25498867e+00  1.79947734e+00  1.81340969e+00  1.99568236e+00\n",
      "  2.34244418e+00  2.09640837e+00  1.61016500e+00  1.59328663e+00\n",
      "  2.46742845e+00  3.14812326e+00  3.72516179e+00  2.89126277e+00\n",
      "  4.15904617e+00  3.03226256e+00  3.41715193e+00  2.51523495e+00\n",
      "  3.50696373e+00  2.72818184e+00  3.43537283e+00  4.09871292e+00\n",
      "  3.60850906e+00  3.97087336e+00  3.33359718e+00  3.08749056e+00\n",
      "  2.60610914e+00  2.44115710e+00  2.56819081e+00  4.14943314e+00\n",
      "  3.62765217e+00  3.78065729e+00  3.12838364e+00  2.91494226e+00\n",
      "  2.79762554e+00  2.76505685e+00  2.90968800e+00  3.75433230e+00\n",
      "  3.82943201e+00  2.95867062e+00  3.04363298e+00  3.02930593e+00\n",
      "  3.03763628e+00  2.91899109e+00  3.02390647e+00  3.67811871e+00\n",
      "  4.23282290e+00  3.50368595e+00  2.97573519e+00  3.75129914e+00\n",
      "  3.49577570e+00  3.14283705e+00  2.73268127e+00  3.67353010e+00\n",
      "  3.98573256e+00  3.79772377e+00  3.96517777e+00  2.76531649e+00\n",
      "  2.87111139e+00  2.90674663e+00  3.55124211e+00  3.60025024e+00\n",
      "  3.28127813e+00  3.83880186e+00  3.63519502e+00  2.74865270e+00\n",
      "  3.23188829e+00  3.95938921e+00  2.59851432e+00  3.98165011e+00\n",
      "  3.46895456e+00  3.19077373e+00  2.62130785e+00  2.78097868e+00\n",
      "  1.08512461e+00 -9.00492311e-01  5.68494737e-01  3.04986238e-01\n",
      "  3.42270553e-01  5.56141436e-01  1.15587354e+00  5.22296488e-01\n",
      "  1.41771328e+00  1.28811848e+00  7.24537432e-01  9.08988714e-01\n",
      "  1.04636955e+00  8.53285789e-01  6.92588449e-01  7.09632516e-01\n",
      "  1.69784427e-02 -2.84931481e-01  8.43248725e-01  1.15852463e+00\n",
      "  1.13854420e+00  1.24205840e+00  1.07288694e+00  8.95336866e-01\n",
      "  7.34200001e-01 -5.68819225e-01  4.68931317e-01  2.45148182e-01\n",
      "  6.06290102e-01  4.29955214e-01 -2.32503295e-01  3.39008331e-01\n",
      " -4.78312850e-01 -3.75783026e-01 -6.61754489e-01 -2.32308418e-01\n",
      "  2.15770453e-01 -5.91168702e-02 -1.81959778e-01 -3.01503211e-01\n",
      " -5.51092207e-01 -1.15282488e+00 -5.07167816e-01 -4.05064046e-01\n",
      " -5.74979007e-01 -3.19065094e-01 -5.97530186e-01  1.47806019e-01\n",
      " -1.22932971e+00 -8.67736578e-01 -7.93235362e-01 -2.99550354e-01\n",
      " -5.89106143e-01 -5.75690567e-01 -3.05701554e-01 -1.38212383e-01\n",
      " -7.73398876e-01 -7.96818376e-01 -1.00765717e+00 -1.14883602e+00\n",
      " -5.10577023e-01 -2.00586617e-01 -4.64167655e-01 -8.87635723e-02\n",
      " -1.73656905e+00  8.37749124e-01  6.14948630e-01  4.23419029e-01\n",
      "  4.04829383e-01  8.28302577e-02  3.92071158e-03]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(FAILURE_INDICES_PATH, \"r\") as f:\n",
    "    failure_data = json.load(f)\n",
    "\n",
    "# ==========================================\n",
    "ERROR_MAGNITUDE = \"6\"  # Choose from '3', '4', '5', '6' \n",
    "FAILURE_IDX_IN_LIST = 0 # Pick the Nth failure in that category\n",
    "# ==========================================\n",
    "\n",
    "dataset_idx = failure_data[ERROR_MAGNITUDE][FAILURE_IDX_IN_LIST]\n",
    "\n",
    "# Load that specific sample\n",
    "dataset = NPZChessDataset(TEST_PATH)\n",
    "X_sample, y_sample = dataset[dataset_idx]\n",
    "    \n",
    "# Unsqueeze so [775] > [1, 775] as model expects batch\n",
    "X_input = X_sample.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "pred_logits = model(X_input)\n",
    "pred_label = torch.argmax(pred_logits, dim=1).item()\n",
    "\n",
    "# Generate Saliency to see why it predicted the wrong class\n",
    "grads = generate_saliency_map(model, X_input, pred_label)\n",
    "\n",
    "print(grads)\n",
    "\n",
    "# # Reconstruct FEN for visualization\n",
    "# reconstructed_fen = vector_to_fen(X_sample)\n",
    "\n",
    "# print(f\"Analyzing Test Sample Index: {dataset_idx}\")\n",
    "# print(f\"FEN: {reconstructed_fen}\")\n",
    "# print(f\"True Label: {y_sample.item()} | Predicted: {pred_label}\")\n",
    "# print(f\"Error Magnitude: {abs(y_sample.item() - pred_label)}\")\n",
    "\n",
    "# plot_chess_heatmap(grads, fen_str=reconstructed_fen, true_label=y_sample.item(), pred_label=pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b006a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIECE_ACC_LOG = LOGS_DIR / \"accuracy_vs_piece_count.csv\"\n",
    "\n",
    "# def compute_piece_accuracy(model, dataloader):\n",
    "#     print(\"Computing Accuracy vs Piece Count (this may take a minute)...\")\n",
    "#     results = []\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "#             preds = model(X).argmax(dim=1)\n",
    "            \n",
    "#             # X shape is (Batch, 775). First 768 are bitboards.\n",
    "#             # Summing X[:, :768] gives total number of pieces on board for each sample.\n",
    "#             # (Assuming bitboards are 1 for piece, 0 for empty)\n",
    "#             piece_counts = X[:, :768].sum(dim=1).cpu().numpy()\n",
    "#             correct = (preds == y).cpu().numpy()\n",
    "            \n",
    "#             for count, is_correct in zip(piece_counts, correct):\n",
    "#                 results.append({\"piece_count\": int(count), \"correct\": int(is_correct)})\n",
    "                \n",
    "#     df = pd.DataFrame(results)\n",
    "#     # Group by piece count and calc mean accuracy\n",
    "#     summary = df.groupby(\"piece_count\")[\"correct\"].mean().reset_index()\n",
    "#     summary.rename(columns={\"correct\": \"accuracy\"}, inplace=True)\n",
    "#     return summary\n",
    "\n",
    "# # Check if exists\n",
    "# if PIECE_ACC_LOG.exists():\n",
    "#     print(f\"Loading existing log from {PIECE_ACC_LOG}...\")\n",
    "#     df_piece_acc = pd.read_csv(PIECE_ACC_LOG)\n",
    "# else:\n",
    "#     test_loader = get_test_loader()\n",
    "#     df_piece_acc = compute_piece_accuracy(model, test_loader)\n",
    "#     df_piece_acc.to_csv(PIECE_ACC_LOG, index=False)\n",
    "#     print(f\"Saved new log to {PIECE_ACC_LOG}\")\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.lineplot(data=df_piece_acc, x=\"piece_count\", y=\"accuracy\", marker=\"o\")\n",
    "# plt.title(f\"Model Accuracy vs. Number of Pieces ({MODEL_TYPE})\")\n",
    "# plt.xlabel(\"Total Pieces on Board\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27dfa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE_ACC_LOG = LOGS_DIR / \"accuracy_vs_phase.csv\"\n",
    "\n",
    "# def compute_phase_accuracy(model, dataloader):\n",
    "#     print(\"Computing Accuracy vs Game Phase...\")\n",
    "    \n",
    "#     results = []\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             X_dev, y_dev = X.to(DEVICE), y.to(DEVICE)\n",
    "#             preds = model(X_dev).argmax(dim=1)\n",
    "#             correct = (preds == y_dev).cpu().numpy()\n",
    "            \n",
    "#             # Calculate phase score manually from bitboards\n",
    "#             # Sum bits in specific ranges * weights\n",
    "#             X_cpu = X.cpu().numpy()\n",
    "            \n",
    "#             # White + Black Material\n",
    "#             pawns = X_cpu[:, 0:64].sum(1) + X_cpu[:, 384:448].sum(1)\n",
    "#             knights = X_cpu[:, 64:128].sum(1) + X_cpu[:, 448:512].sum(1)\n",
    "#             bishops = X_cpu[:, 128:192].sum(1) + X_cpu[:, 512:576].sum(1)\n",
    "#             rooks = X_cpu[:, 192:256].sum(1) + X_cpu[:, 576:640].sum(1)\n",
    "#             queens = X_cpu[:, 256:320].sum(1) + X_cpu[:, 640:704].sum(1)\n",
    "            \n",
    "#             # Total material score (Standard: Max ~78 excl Kings)\n",
    "#             material = (pawns * 1) + (knights * 3) + (bishops * 3) + (rooks * 5) + (queens * 9)\n",
    "            \n",
    "#             # Binning: Opening (>60), Mid (30-60), End (<30) - adjust as needed\n",
    "#             for mat, is_corr in zip(material, correct):\n",
    "#                 results.append({\"material_score\": mat, \"correct\": int(is_corr)})\n",
    "\n",
    "#     df = pd.DataFrame(results)\n",
    "#     # Binning\n",
    "#     df['phase_bin'] = pd.cut(df['material_score'], bins=[0, 30, 60, 200], labels=['Endgame', 'Middlegame', 'Opening'])\n",
    "#     summary = df.groupby(\"phase_bin\")[\"correct\"].mean().reset_index()\n",
    "#     return summary\n",
    "\n",
    "# if PHASE_ACC_LOG.exists():\n",
    "#     print(f\"Loading existing log from {PHASE_ACC_LOG}...\")\n",
    "#     df_phase = pd.read_csv(PHASE_ACC_LOG)\n",
    "# else:\n",
    "#     test_loader = get_test_loader()\n",
    "#     df_phase = compute_phase_accuracy(model, test_loader)\n",
    "#     df_phase.to_csv(PHASE_ACC_LOG, index=False)\n",
    "#     print(f\"Saved new log to {PHASE_ACC_LOG}\")\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.barplot(data=df_phase, x=\"phase_bin\", y=\"correct\", palette=\"viridis\")\n",
    "# plt.title(\"Accuracy vs Game Phase\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.ylim(0, 1.0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f25947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSITIONAL_LOG = LOGS_DIR / \"positional_report.json\"\n",
    "\n",
    "# def generate_positional_report(model, dataloader):\n",
    "#     print(\"Generating Positional Understanding Report...\")\n",
    "#     model.eval()\n",
    "    \n",
    "#     equal_positions = 0\n",
    "#     correct_equal = 0\n",
    "#     predicted_draw_when_not = 0\n",
    "#     total_samples = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "#             preds = model(X).argmax(dim=1)\n",
    "            \n",
    "#             # Focus on Class 3 (Equal)\n",
    "#             is_equal_label = (y == 3)\n",
    "#             equal_positions += is_equal_label.sum().item()\n",
    "#             correct_equal += (preds[is_equal_label] == 3).sum().item()\n",
    "            \n",
    "#             # Where model predicted Draw (3) but label was NOT 3\n",
    "#             is_predicted_equal = (preds == 3)\n",
    "#             # Mask out where it was actually equal\n",
    "#             false_alarms = is_predicted_equal & (~is_equal_label)\n",
    "#             predicted_draw_when_not += false_alarms.sum().item()\n",
    "            \n",
    "#             total_samples += len(y)\n",
    "\n",
    "#     report = {\n",
    "#         \"total_samples_analyzed\": total_samples,\n",
    "#         \"total_equal_positions\": equal_positions,\n",
    "#         \"accuracy_on_equal_positions\": correct_equal / equal_positions if equal_positions > 0 else 0,\n",
    "#         \"draw_blindness_rate\": 1 - (correct_equal / equal_positions) if equal_positions > 0 else 0,\n",
    "#         \"draw_hallucination_count\": predicted_draw_when_not\n",
    "#     }\n",
    "#     return report\n",
    "\n",
    "# if POSITIONAL_LOG.exists():\n",
    "#     with open(POSITIONAL_LOG, \"r\") as f:\n",
    "#         report = json.load(f)\n",
    "#     print(\"Loaded existing positional report.\")\n",
    "# else:\n",
    "#     test_loader = get_test_loader()\n",
    "#     report = generate_positional_report(model, test_loader)\n",
    "#     with open(POSITIONAL_LOG, \"w\") as f:\n",
    "#         json.dump(report, f, indent=4)\n",
    "#     print(\"Created new positional report.\")\n",
    "\n",
    "# print(json.dumps(report, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "chessenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
