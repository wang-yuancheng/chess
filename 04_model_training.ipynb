{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b389472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ed291a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" \n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39910bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZChessDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        self.data = np.load(npz_path, mmap_mode=\"r\")\n",
    "        self.X = self.data[\"X\"]\n",
    "        self.y = self.data[\"y\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32) # float32 for NNs\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long) # long for CrossEntropyLoss\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15af8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(dataset=NPZChessDataset(\"chess_bitboards_train.npz\"), \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=NPZChessDataset(\"chess_bitboards_val.npz\"), \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=NPZChessDataset(\"chess_bitboards_test.npz\"), \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1afc6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg batch load time: 0.0008795595169067383\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If this value increases significantly in future (e.g. >10-20 ms per batch),\n",
    "that means that CPU/GPU is waiting on the DataLoader.\n",
    "check Dataset code or consider using num_workers > 0 for parallel loading\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "loader = DataLoader(NPZChessDataset(\"chess_bitboards_train.npz\"), \n",
    "                    batch_size=256, \n",
    "                    shuffle=True, \n",
    "                    num_workers=0)\n",
    "\n",
    "start = time.time()\n",
    "for i, (X, y) in enumerate(loader):\n",
    "    if i == 100:  # measure 100 batches\n",
    "        break\n",
    "print(\"Avg batch load time:\", (time.time() - start) / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5bdf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: torch.Size([32, 768]) dtype: torch.float32\n",
      "y batch shape: torch.Size([32]) dtype: torch.int64 classes in batch: [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "Xb, yb = next(iter(train_dataloader))\n",
    "print(\"X batch shape:\", Xb.shape, \"dtype:\", Xb.dtype)\n",
    "print(\"y batch shape:\", yb.shape, \"dtype:\", yb.dtype, \"classes in batch:\", yb.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionLabel(Enum):\n",
    "    WHITE_WINNING = 0\n",
    "    WHITE_DECISIVE = 1\n",
    "    WHITE_BETTER = 2\n",
    "    EQUAL = 3\n",
    "    BLACK_BETTER = 4\n",
    "    BLACK_DECISIVE = 5\n",
    "    BLACK_WINNING = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6db7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTestModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d947aea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RandomTestModel                          [32, 7]                   --\n",
       "├─Sequential: 1-1                        [32, 7]                   --\n",
       "│    └─Linear: 2-1                       [32, 100]                 76,900\n",
       "│    └─ReLU: 2-2                         [32, 100]                 --\n",
       "│    └─Linear: 2-3                       [32, 100]                 10,100\n",
       "│    └─ReLU: 2-4                         [32, 100]                 --\n",
       "│    └─Linear: 2-5                       [32, 7]                   707\n",
       "==========================================================================================\n",
       "Total params: 87,707\n",
       "Trainable params: 87,707\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.35\n",
       "Estimated Total Size (MB): 0.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "model = RandomTestModel(input_shape=768,\n",
    "                  hidden_units=100,\n",
    "                  output_shape=7) # 7 classes\n",
    "summary(model, input_size=(32, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Performs one training epoch for the given model.\n",
    "    Returns the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device) # X and Y are both shape (BATCH_SIZE,)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy metrics\n",
    "        \"\"\"softmax and argmax dim=1 because tensor of shape (batchsize, num_classes)\"\"\"\n",
    "        y_pred_class = torch.argmax(y_pred, dim=-1) # y_pred_class.shape = (BATCH_SIZE,)\n",
    "        train_accuracy += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(dataloader)\n",
    "\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56112dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the given dataloader without gradient updates.\n",
    "    Dataloader should either be the validation or test dataloader.\n",
    "    Returns the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_accuracy = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy metrics\n",
    "            test_pred_labels = torch.argmax(test_pred, dim=1)\n",
    "            test_accuracy += (test_pred_labels == y).sum().item()/len(test_pred_labels)\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_accuracy = test_accuracy / len(dataloader)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121015af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
